# coding=utf-8
# Copyright 2022 Meta Platforms, Inc. and The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""" PyTorch Mask2Former model."""

import math
import warnings
from dataclasses import dataclass
from typing import List, Dict, Tuple, Iterable, Type, Union, Callable, Optional

import numpy as np
from numpy import ndarray
import torch
from torch import Tensor, nn

from transformers.activations import ACT2FN
from transformers.file_utils import (
    ModelOutput,
    add_start_docstrings,
    add_start_docstrings_to_model_forward,
    is_scipy_available,
    replace_return_docstrings,
    requires_backends,
)
from transformers.modeling_outputs import BaseModelOutput, BaseModelOutputWithCrossAttentions
from transformers.modeling_utils import PreTrainedModel
from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_1
from transformers.utils import is_accelerate_available, logging
from transformers.utils.backbone_utils import load_backbone
from .configuration_mask2former import Mask2FormerConfig


# Newly Added 
import cv2
import random
import torchvision
import torch_dct as dct 
import torch.nn.functional as F
from torch.nn.functional import smooth_l1_loss 
from torch.nn.functional import binary_cross_entropy_with_logits as sigmoid_bce_loss
from torchvision.ops import complete_box_iou_loss, distance_box_iou_loss, generalized_box_iou_loss, sigmoid_focal_loss
from models.resnet import *
from einops import rearrange



if is_scipy_available():
    from scipy.optimize import linear_sum_assignment

if is_accelerate_available():
    from accelerate import PartialState
    from accelerate.utils import reduce

logger = logging.get_logger(__name__)


_CONFIG_FOR_DOC = "Mask2FormerConfig"
_CHECKPOINT_FOR_DOC = "facebook/mask2former-swin-small-coco-instance"
_IMAGE_PROCESSOR_FOR_DOC = "Mask2FormerImageProcessor"


@dataclass
class Mask2FormerPixelDecoderOutput(ModelOutput):
    """
    Mask2Former's pixel decoder module output, practically a Multi-Scale Deformable Attention based decoder. It returns
    the multiscale features.

    Args:
        multi_scale_features (`tuple(torch.FloatTensor)`):
            Tuple of multi-scale features of scales [1/8, 1/16, 1/32] and shape `(batch_size, num_channels, height,
            width)`from the Multi-Scale Deformable Attenntion based Pixel Decoder.
        attentions (`tuple(torch.FloatTensor)`, *optional*):
            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,
            sequence_length)`. Attentions weights from pixel decoder. Returned when `output_attentions=True` is passed
            or when `config.output_attentions=True`
    """

    multi_scale_features: Tuple[torch.FloatTensor] = None
    attentions: Optional[Tuple[torch.FloatTensor]] = None


@dataclass
class Mask2FormerPixelLevelModuleOutput(ModelOutput):
    """
    Mask2Former's pixel level module output. It returns the output of the encoder (optional) and all hidden states
    (multi-scale features) from the `decoder`. By default, the `encoder` is a Swin Backbone and the `decoder` is a
    Multi-Scale Deformable Attention based decoder.

    The `decoder_hidden_states` refer to multi-scale feature maps produced using **multi-scaling strategy** defined in the paper.

    Args:
        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*):
            Tuple of `torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`. Hidden states (also
            called feature maps) of the model at the output of each stage. Returned if output_hidden_states is set to
            True.
        decoder_hidden_states (`tuple(torch.FloatTensor)`):
            Tuple of `torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`. Hidden states (also
            called feature maps) of the model at the output of each stage.
        decoder_mask_features (`torch.FloatTensor`):
            A `torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`. This tensor contains the mask features generated by the decoder, 
            which are used for predicting the final segmentation masks.
    """

    encoder_hidden_states: Tuple[torch.FloatTensor] = None
    decoder_hidden_states: Tuple[torch.FloatTensor] = None
    decoder_mask_features: torch.FloatTensor = None


@dataclass
class Mask2FormerMaskedAttentionDecoderOutput(BaseModelOutputWithCrossAttentions):
    """
    Base class for outputs of the Transformer decoder. This class adds two attributes to
    BaseModelOutputWithCrossAttentions for mask predictions logits and a tuple of intermediate decoder activations,
    i.e. the output of each decoder layer, each of them gone through a layernorm.

    Args:
        all_instance_states (`tuple(torch.FloatTensor)`, *optional*):
            Tuple of `torch.FloatTensor` representing instance-level hidden states from different stages of the decoder.
            Shape is `(batch_size, num_instances, hidden_size)`. Returned when instance-level information is required.
        all_semantic_states (`tuple(torch.FloatTensor)`, *optional*):
            Tuple of `torch.FloatTensor` representing semantic-level hidden states from different stages of the decoder.
            Shape is `(batch_size, num_classes, hidden_size)`. Returned when semantic-level information is required.
        all_instance_masks (`tuple(torch.FloatTensor)` of shape `(batch_size, num_instances, height, width)`):
            Tuple of instance mask predictions from the model.
        all_semantic_masks (`tuple(torch.FloatTensor)` of shape `(batch_size, num_classes, height, width)`):
            Tuple of semantic mask predictions from the model.
        all_bbox_predictions (`tuple(torch.FloatTensor)` of shape `(batch_size, num_instances, 4)`):
            Tuple of bounding box predictions from the model.
        all_cate_predictions (`tuple(torch.FloatTensor)` of shape `(batch_size, num_instances, num_classes)`):
            Tuple of category predictions from the model.

    """
    
    all_instance_states: Optional[Tuple[torch.FloatTensor]] = None
    all_semantic_states: Optional[Tuple[torch.FloatTensor]] = None
    all_instance_masks: Tuple[torch.FloatTensor] = None
    all_semantic_masks: Tuple[torch.FloatTensor] = None
    all_bbox_predictions: Tuple[torch.FloatTensor] = None
    all_cate_predictions: Tuple[torch.FloatTensor] = None


@dataclass
class Mask2FormerModelOutput(ModelOutput):
    """
    Class for outputs of [`Mask2FormerModel`]. This class returns all the needed hidden states to compute the logits.

    Args:
        pixel_encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*):
            Tuple of `torch.FloatTensor` representing hidden states from the pixel encoder, with shape 
            `(batch_size, num_channels, height, width)`. Returned when `output_hidden_states=True` is passed.
        pixel_decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*):
            Tuple of `torch.FloatTensor` representing hidden states from the pixel decoder, with shape 
            `(batch_size, num_channels, height, width)`. Returned when `output_hidden_states=True` is passed.
        pixel_decoder_mask_features (`torch.FloatTensor`, *optional*):
            Feature maps for mask prediction generated by the pixel decoder, with shape 
            `(batch_size, num_channels, height, width)`.
        transformer_decoder_instance_states (`tuple(torch.FloatTensor)`, *optional*):
            Tuple of `torch.FloatTensor` representing instance-level hidden states from the transformer decoder, with shape 
            `(batch_size, num_instances, hidden_size)`.
        transformer_decoder_semantic_states (`tuple(torch.FloatTensor)`, *optional*):
            Tuple of `torch.FloatTensor` representing semantic-level hidden states from the transformer decoder, with shape 
            `(batch_size, num_classes, hidden_size)`.
        transformer_decoder_instance_masks (`tuple(torch.FloatTensor)`):
            Tuple of `torch.FloatTensor` representing instance mask predictions, with shape 
            `(batch_size, num_instances, height, width)`.
        transformer_decoder_semantic_masks (`tuple(torch.FloatTensor)`):
            Tuple of `torch.FloatTensor` representing semantic mask predictions, with shape 
            `(batch_size, num_classes, height, width)`.
        transformer_decoder_bbox_predictions (`tuple(torch.FloatTensor)`):
            Tuple of `torch.FloatTensor` representing bounding box predictions, with shape 
            `(batch_size, num_instances, 4)`.
        transformer_decoder_cate_predictions (`tuple(torch.FloatTensor)`):
            Tuple of `torch.FloatTensor` representing category predictions, with shape 
            `(batch_size, num_instances, num_classes)`.
    """

    pixel_encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None
    pixel_decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None
    pixel_decoder_mask_features: Optional[Tuple[torch.FloatTensor]] = None
    transformer_decoder_instance_states: Optional[Tuple[torch.FloatTensor]] = None
    transformer_decoder_semantic_states: Optional[Tuple[torch.FloatTensor]] = None
    transformer_decoder_instance_masks: Tuple[torch.FloatTensor] = None
    transformer_decoder_semantic_masks: Tuple[torch.FloatTensor] = None
    transformer_decoder_bbox_predictions: Tuple[torch.FloatTensor] = None
    transformer_decoder_cate_predictions: Tuple[torch.FloatTensor] = None
    

@dataclass
class Mask2FormerForUniversalSegmentationOutput(ModelOutput):
    """
    Class for outputs of [`Mask2FormerForUniversalSegmentationOutput`].

    Args:
        loss (`torch.FloatTensor`, *optional*):
            The computed loss, returned when labels are present.
        pixel_encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*):
            Tuple of `torch.FloatTensor` representing hidden states from the pixel encoder, with shape `(batch_size, num_channels, height, width)`. Returned when `output_hidden_states=True` is passed.
        pixel_decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*):
            Tuple of `torch.FloatTensor` representing hidden states from the pixel decoder, with shape `(batch_size, num_channels, height, width)`. Returned when `output_hidden_states=True` is passed.
        pixel_decoder_mask_features (`torch.FloatTensor`, *optional*):
            Feature maps for mask prediction generated by the pixel decoder, with shape `(batch_size, num_channels, height, width)`.
        transformer_decoder_instance_states (`tuple(torch.FloatTensor)`, *optional*):
            Tuple of `torch.FloatTensor` representing instance-level hidden states from the transformer decoder, with shape `(batch_size, num_instances, hidden_size)`.
        transformer_decoder_semantic_states (`tuple(torch.FloatTensor)`, *optional*):
            Tuple of `torch.FloatTensor` representing semantic-level hidden states from the transformer decoder, with shape `(batch_size, num_classes, hidden_size)`.
        transformer_decoder_instance_masks (`tuple(torch.FloatTensor)`):
            Tuple of `torch.FloatTensor` representing instance mask predictions, with shape `(batch_size, num_instances, height, width)`.
        transformer_decoder_semantic_masks (`tuple(torch.FloatTensor)`):
            Tuple of `torch.FloatTensor` representing semantic mask predictions, with shape `(batch_size, num_classes, height, width)`.
        transformer_decoder_bbox_predictions (`tuple(torch.FloatTensor)`):
            Tuple of `torch.FloatTensor` representing bounding box predictions, with shape `(batch_size, num_instances, 4)`.
        transformer_decoder_cate_predictions (`tuple(torch.FloatTensor)`):
            Tuple of `torch.FloatTensor` representing category predictions, with shape `(batch_size, num_instances, num_classes)`.
    """
    
    loss: Optional[torch.FloatTensor] = None
    pixel_encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None
    pixel_decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None
    pixel_decoder_mask_features: Optional[Tuple[torch.FloatTensor]] = None
    transformer_decoder_instance_states: Optional[Tuple[torch.FloatTensor]] = None
    transformer_decoder_semantic_states: Optional[Tuple[torch.FloatTensor]] = None
    transformer_decoder_instance_masks: Tuple[torch.FloatTensor] = None
    transformer_decoder_semantic_masks: Tuple[torch.FloatTensor] = None
    transformer_decoder_bbox_predictions: Tuple[torch.FloatTensor] = None
    transformer_decoder_cate_predictions: Tuple[torch.FloatTensor] = None


# Sample grid points according to image size
def sample_grid_points(hei, wid, is_offset=False):
    """
    Samples grid points from a uniform grid with optional offset.

    Args:
        hei (int): The height of the grid to sample from.
        wid (int): The width of the grid to sample from.
        is_offset (bool, optional): If True, adds a random offset in the range [-0.5, 0.5] to each coordinate. Defaults to False.

    Returns:
        coords (torch.Tensor): A tensor of shape `(1, N, 2)` where `N` is the total number of sampled points (`hei * wid`),
            and each point consists of normalized (x, y) coordinates.
    """
    
    ref_y, ref_x = torch.meshgrid(torch.linspace(0.5, hei-0.5, hei), torch.linspace(0.5, wid-0.5, wid), indexing="ij")
    
    if is_offset:
        ref_y = ref_y + torch.rand_like(ref_y) - 0.5
        ref_x = ref_x + torch.rand_like(ref_x) - 0.5

    ref_y = ref_y.reshape(-1)[None] / hei
    ref_x = ref_x.reshape(-1)[None] / wid
    coords = torch.stack((ref_x, ref_y), -1) # (1, N, 2)

    return coords


# Adapted from https://github.com/facebookresearch/detectron2/blob/main/projects/PointRend/point_rend/point_features.py
def sample_point(input_features: Tensor, point_coordinates: Tensor, add_dim=False, **kwargs) -> Tensor:
    """
    Samples features at specified point coordinates from the input feature map using bilinear interpolation.

    Args:
        input_features (Tensor): A tensor of shape `(B, C, H, W)` where `B` is the batch size, `C` is the number of channels,
                                 `H` is the height and `W` is the width of the feature map.
        point_coordinates (Tensor): A tensor of shape `(B, N, 2)` or `(B, N, 1, 2)` where `N` is the number of points to sample,
                                    and each point consists of normalized (x, y) coordinates in the range [0, 1].
        add_dim (bool, optional): If True and `point_coordinates` has shape `(B, N, 2)`, an additional dimension will be added
                                  to make it `(B, N, 1, 2)`. Defaults to False.
        **kwargs: Additional arguments passed to `torch.nn.functional.grid_sample`.

    Returns:
        point_features (Tensor): A tensor of shape `(B, C, N)` or `(B, C, N, 1)` containing the interpolated feature values
                                 for each sampled point. The exact shape depends on the value of `add_dim` and whether the 
                                 `grid_sample` operation retains the last dimension.
    """
    
    if point_coordinates.dim() == 3:
        add_dim = True
        point_coordinates = point_coordinates.unsqueeze(2)  # (B, N, 2) -> (B, N, 1, 2)

    # use nn.function.grid_sample to get features for points in `point_coordinates` via bilinear interpolation
    point_features = nn.functional.grid_sample(input_features, 2.0 * point_coordinates - 1.0, **kwargs) # (B, C, N, 1)
    if add_dim:
        point_features = point_features.squeeze(3)  # (B, C, N, 1) -> # (B, C, N)

    return point_features


# Copied from transformers.models.maskformer.modeling_maskformer.sigmoid_dice_loss
def sigmoid_dice_loss(inputs: Tensor, labels: Tensor, reduction="none") -> Tensor:
    """
    Computes the Sigmoid Dice Loss between the inputs and the targets.

    Args:
        inputs (Tensor): A tensor of shape `(B, *)` where `B` is the batch size and `*` indicates any number of additional
            dimensions. This tensor contains raw, unnormalized scores for each class before applying sigmoid.
        labels (Tensor): A tensor of the same shape as `inputs` `(B, *)`. It contains binary ground truth values for each
            element in `inputs`.
        reduction (str, optional): Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'.
            - 'none': no reduction will be applied.
            - 'mean': the sum of the output will be divided by the number of elements in the output.
            - 'sum': the output will be summed.
            Default is 'none'.

    Returns:
        Tensor: The computed Sigmoid Dice Loss. The shape of the returned tensor depends on the `reduction` argument:
            - If `reduction` is 'none', returns a tensor of shape `(B, *)`, representing the loss for each element.
            - If `reduction` is 'mean' or 'sum', returns a scalar tensor.
    """
    
    probs = inputs.sigmoid()
    numerator = 2 * (probs * labels).sum(-1)
    denominator = probs.sum(-1) + labels.sum(-1)
    loss = 1 - (numerator + 1) / (denominator + 1)
    
    # Check reduction option and return loss accordingly
    if reduction == "none":
        pass
    elif reduction == "mean":
        loss = loss.mean()
    elif reduction == "sum":
        loss = loss.sum()
    else:
        raise ValueError(
            f"Invalid Value for arg 'reduction': '{reduction} \n Supported reduction modes: 'none', 'mean', 'sum'"
        )
        
    return loss


@torch.no_grad()
def pair_wise_sigmoid_dice_loss(inputs: Tensor, labels: Tensor) -> Tensor:
    """
    Computes the pairwise Sigmoid Dice Loss between each element in the inputs and each element in the labels.
    
    Args:
        inputs (torch.Tensor): A tensor of shape `(num_queries, *)` where `num_queries` represents the number of queries,
                               and `*` indicates any number of additional dimensions. This tensor contains raw, unnormalized 
                               scores for each class before applying sigmoid activation.
        labels (torch.Tensor): A tensor of the same shape as `inputs` `(num_references, *)` where `num_references` should 
                               ideally match `num_queries` to represent corresponding binary ground truth values for each 
                               element in `inputs`. Here, `*` indicates any number of additional dimensions that must be 
                               compatible with `inputs` for element-wise operations.

    Returns:
        torch.Tensor: A tensor of shape `(num_queries, num_references)` representing the pairwise Sigmoid Dice Loss between 
                      each query and each reference sample. The loss values are normalized to the range [0, 1] for consistency.
    """
    
    inputs = inputs.sigmoid()
    numerator = 2 * torch.matmul(inputs, labels.T)
    denominator = inputs.sum(-1)[:, None] + labels.sum(-1)[None, :]
    loss_matrix = 1 - (numerator + 1) / (denominator + 1)
    
    max_value = loss_matrix.max()
    min_value = loss_matrix.min()
    loss_matrix = (loss_matrix - min_value) / max(max_value - min_value, 1e-10)

    return loss_matrix


@torch.no_grad()
def pair_wise_sigmoid_bce_loss(inputs: Tensor, labels: Tensor) -> Tensor:
    """
    Computes the pairwise sigmoid binary cross entropy loss between inputs and labels.

    Args:
        inputs (torch.Tensor): A tensor of shape `(num_queries, *)` where `num_queries` represents the number of queries,
                               and `*` indicates any number of additional dimensions. This tensor contains raw, unnormalized 
                               scores for each class before applying sigmoid activation.
        labels (torch.Tensor): A tensor of the same shape as `inputs` `(num_references, *)` where `num_references` should 
                               ideally match `num_queries` to represent corresponding binary ground truth values for each 
                               element in `inputs`. Here, `*` indicates any number of additional dimensions that must be 
                               compatible with `inputs` for element-wise operations.

    Returns:
        torch.Tensor: A tensor of shape `(num_queries, num_references)` representing the pairwise Sigmoid Dice Loss between 
                      each query and each reference sample. The loss values are normalized to the range [0, 1] for consistency.
    """
    
    N, D = inputs.shape

    criterion = nn.BCEWithLogitsLoss(reduction="none")
    cross_entropy_loss_pos = criterion(inputs, torch.ones_like(inputs))
    cross_entropy_loss_neg = criterion(inputs, torch.zeros_like(inputs))

    loss_pos = torch.matmul(cross_entropy_loss_pos / D, labels.T)
    loss_neg = torch.matmul(cross_entropy_loss_neg / D, (1 - labels).T)
    loss_matrix = loss_pos + loss_neg
    
    max_value = loss_matrix.max()
    min_value = loss_matrix.min()
    loss_matrix = (loss_matrix - min_value) / max(max_value - min_value, 1e-10)

    return loss_matrix


@torch.no_grad()
def pair_wise_sigmoid_focal_loss(inputs: Tensor, labels: Tensor, alpha=0.25, gamma=2) -> Tensor:
    """
    Computes the pairwise sigmoid focal loss between inputs and labels.

    Args:
        inputs (torch.Tensor): A tensor of shape `(num_queries, *)` where `num_queries` represents the number of queries,
                               and `*` indicates any number of additional dimensions. This tensor contains raw, unnormalized 
                               scores for each class before applying sigmoid activation.
        labels (torch.Tensor): A tensor of the same shape as `inputs` `(num_references, *)` where `num_references` should 
                               ideally match `num_queries` to represent corresponding binary ground truth values for each 
                               element in `inputs`. Here, `*` indicates any number of additional dimensions that must be 
                               compatible with `inputs` for element-wise operations.

    Returns:
        torch.Tensor: A tensor of shape `(num_queries, num_references)` representing the pairwise Sigmoid Dice Loss between 
                      each query and each reference sample. The loss values are normalized to the range [0, 1] for consistency.
    """
    
    N, D = inputs.shape

    focal_loss_pos = sigmoid_focal_loss(inputs, torch.ones_like(inputs), alpha=alpha, gamma=gamma, reduction="none")
    focal_loss_neg = sigmoid_focal_loss(inputs, torch.zeros_like(inputs), alpha=alpha, gamma=gamma, reduction="none")

    loss_pos = torch.matmul(focal_loss_pos / D, labels.T)
    loss_neg = torch.matmul(focal_loss_neg / D, (1 - labels).T)
    loss_matrix = loss_pos + loss_neg

    max_value = loss_matrix.max()
    min_value = loss_matrix.min()
    loss_matrix = (loss_matrix - min_value) / max(max_value - min_value, 1e-10)

    return loss_matrix


@torch.no_grad()
def pair_wise_bbox_loss(inputs: Tensor, labels: Tensor) -> Tensor:
    """
    Computes the pairwise bounding box loss between each bounding box in `inputs` and each bounding box in `labels`.

    Args:
        inputs (torch.Tensor): A tensor of shape `(M, D)` where `M` is the number of predicted bounding boxes and `D` is the 
                               dimensionality of each bounding box (typically 4 for coordinates). This tensor contains the 
                               predicted bounding box parameters.
        labels (torch.Tensor): A tensor of shape `(N, D)` representing the ground truth bounding boxes corresponding to the 
                               predictions in `inputs`. Here, `N` is the number of ground truth boxes, and `D` should match 
                               the dimensionality of the predicted boxes.

    Returns:
        torch.Tensor: A tensor of shape `(M, N)` representing the pairwise bounding box loss between each predicted bounding 
                      box and each ground truth bounding box. The loss values are normalized to the range [0, 1] for consistency.
    """
    
    M, D = inputs.shape
    N, D = labels.shape
    inputs = inputs[:,None,:].expand(M, N, D)
    labels = labels[None,:,:].expand(M, N, D)
    loss_matrix = smooth_l1_loss(inputs, labels, reduction="none").mean(dim=-1)

    max_value = loss_matrix.max()
    min_value = loss_matrix.min()
    loss_matrix = (loss_matrix - min_value) / max(max_value - min_value, 1e-10)

    return loss_matrix


@torch.no_grad()
def pair_wise_diou_loss(inputs: Tensor, labels: Tensor) -> Tensor:
    """
    Computes the pairwise Distance-IoU (DIoU) loss between each bounding box in `inputs` and each bounding box in `labels`.

    Args:
        inputs (torch.Tensor): A tensor of shape `(M, D)` where `M` is the number of predicted bounding boxes and `D` is the 
                               dimensionality of each bounding box (typically 4 for coordinates). This tensor contains the 
                               predicted bounding box parameters.
        labels (torch.Tensor): A tensor of shape `(N, D)` representing the ground truth bounding boxes corresponding to the 
                               predictions in `inputs`. Here, `N` is the number of ground truth boxes, and `D` should match 
                               the dimensionality of the predicted boxes.

    Returns:
        torch.Tensor: A tensor of shape `(M, N)` representing the pairwise DIoU loss between each predicted bounding box and 
                      each ground truth bounding box. The loss values are normalized to the range [0, 1] for consistency.
    """
    
    M, D = inputs.shape
    N, D = labels.shape
    inputs = inputs[:,None,:].expand(M, N, D)
    labels = labels[None,:,:].expand(M, N, D)
    loss_matrix = distance_box_iou_loss(inputs, labels, reduction="none")
    
    max_value = loss_matrix.max()
    min_value = loss_matrix.min()
    loss_matrix = (loss_matrix - min_value) / max(max_value - min_value, 1e-10)

    return loss_matrix


# Adapted from https://github.com/facebookresearch/Mask2Former/blob/main/mask2former/modeling/matcher.py
class Mask2FormerHungarianMatcher(nn.Module):
    """This class computes an assignment between the labels and the predictions of the network.

    For efficiency reasons, the labels don't include the no_object. Because of this, in general, there are more
    predictions than labels. In this case, we do a 1-to-1 matching of the best predictions, while the others are
    un-matched (and thus treated as non-objects).
    """

    def __init__(self, config: Mask2FormerConfig,):
        super().__init__()
        
        self.weight_class = 1.0
        self.weight_mask  = 2.0
        self.weight_bbox  = 2.0
    
    
    def repeated_match(self, cost_matrix, max_repeat_num=1):
        all_row_ids, all_col_ids = [], []
        max_matches = math.ceil(cost_matrix.size(0) / cost_matrix.size(1))
        max_matches = min(max_matches, max_repeat_num)

        cost_matrix = torch.clamp(cost_matrix, min=-1e10, max=1e10)
        for num_matches in range(max_matches):
            row_ids, col_ids = linear_sum_assignment(cost_matrix.cpu().numpy())
            all_row_ids.append(row_ids)
            all_col_ids.append(col_ids)
            cost_matrix[row_ids, :] = 1e12
        
        all_row_ids = np.concatenate(all_row_ids, axis=0)
        all_col_ids = np.concatenate(all_col_ids, axis=0)
        # print(all_row_ids.shape, all_col_ids.shape, cost_matrix.size())
        
        return all_row_ids, all_col_ids


    @torch.no_grad()
    def forward(
        self,
        class_names: List[list[str]],
        masks_queries_logits: Tensor,
        boxes_queries_logits: Tensor,
        class_queries_logits: Tensor,
        mask_labels:  Tensor,
        bbox_labels:  Tensor,
        class_labels: Tensor,
    ) -> List[Tuple[Tensor]]:
        """
        Computes an assignment between the predicted outputs and the ground truth labels using a cost matrix.
        
        Parameters:
            - class_names (List[List[str]]): A list of lists containing class names for each batch item. Each sublist corresponds to one image's class names.
            - masks_queries_logits (torch.Tensor): Tensor of shape `(batch_size, num_queries, height, width)`, representing predicted mask logits for each query.
            - boxes_queries_logits (torch.Tensor): Tensor of shape `(batch_size, num_queries, 4)`, representing predicted bounding box coordinates for each query.
            - class_queries_logits (torch.Tensor): Tensor of shape `(batch_size, num_queries, num_classes)`, representing predicted class probabilities for each query.
            - mask_labels (torch.Tensor): Tensor of shape `(batch_size, num_instances, height, width)`, representing ground truth masks.
            - bbox_labels (torch.Tensor): Tensor of shape `(batch_size, num_instances, 4)`, representing ground truth bounding box coordinates.
            - class_labels (torch.Tensor): Tensor of shape `(batch_size, num_instances)`, representing ground truth class indices.

        Returns:
            - matched_indices (List[Tuple[torch.Tensor]]): A list of tuples, where each tuple contains two tensors representing matched indices between predictions and ground truths.
              The first tensor in each tuple represents the indices of predictions, and the second tensor represents the corresponding ground truth indices.
        """
        indices: List[Tuple[np.array]] = []
        indices_repeated: List[Tuple[np.array]] = []

        # iterate through batch size
        batch_size = class_queries_logits.shape[0]
        for i in range(batch_size):
            cost_matrix = 0
            if class_queries_logits is not None:
                num_labels = len(class_names[i])
                pred_probs = class_queries_logits[i][:,:num_labels].softmax(-1)
                cost_class = 1 - pred_probs[:, class_labels[i]]

                max_value = cost_class.max()
                min_value = cost_class.min()
                cost_class = (cost_class - min_value) / max(max_value - min_value, 1e-10)
                cost_matrix += self.weight_class * cost_class

            if masks_queries_logits is not None:
                pred_mask = masks_queries_logits[i][:,None]
                true_mask = mask_labels[i][:,None].to(pred_mask)

                #point_coordinates = torch.rand(1, self.num_points, 2, device=pred_mask.device)
                point_coordinates = sample_grid_points(pred_mask.shape[2], pred_mask.shape[3], is_offset=True)

                true_coordinates = point_coordinates.expand(true_mask.shape[0], point_coordinates.shape[1], point_coordinates.shape[2]).to(pred_mask.device)
                true_mask = sample_point(true_mask, true_coordinates, align_corners=False).squeeze(1)
                
                pred_coordinates = point_coordinates.expand(pred_mask.shape[0], point_coordinates.shape[1], point_coordinates.shape[2]).to(pred_mask.device)
                pred_mask = sample_point(pred_mask, pred_coordinates, align_corners=False).squeeze(1)

                cost_dice  = pair_wise_sigmoid_dice_loss(pred_mask, true_mask)
                cost_focal = pair_wise_sigmoid_focal_loss(pred_mask, true_mask)
                cost_mask = (cost_dice + cost_focal) / 2
                cost_matrix += self.weight_mask * cost_mask
            
            if boxes_queries_logits is not None:
                cost_sml1 = pair_wise_bbox_loss(boxes_queries_logits[i], bbox_labels[i])
                cost_diou = pair_wise_diou_loss(boxes_queries_logits[i], bbox_labels[i])
                cost_bbox = (cost_sml1 + cost_diou) / 2
                cost_matrix += self.weight_bbox * cost_bbox
 
            # eliminate infinite values in cost_matrix to avoid the error ``ValueError: cost matrix is infeasible``
            cost_matrix = torch.clamp(cost_matrix, min=-1e10, max=1e10)
            cost_matrix = torch.nan_to_num(cost_matrix, nan=1e10, posinf=1e10, neginf=-1e10)
            
            # do the assigmented using the hungarian algorithm in scipy
            assigned_indices: Tuple[np.array] = linear_sum_assignment(cost_matrix.cpu())
            # assigned_indices: Tuple[np.array] = self.repeated_match(cost_matrix.cpu(), max_repeat_num=5)
            indices.append(assigned_indices)

        # It could be stacked in one tensor
        matched_indices = [[torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)] for i, j in indices]

        return matched_indices
    

# Adapted from https://github.com/facebookresearch/Mask2Former/blob/main/mask2former/modeling/criterion.py
class Mask2FormerLoss(nn.Module):
    def __init__(self, config: Mask2FormerConfig):
        """
        The Mask2Former Loss. The loss is computed very similar to DETR. The process happens in two steps: 1) we
        compute hungarian assignment between ground truth masks and the outputs of the model 2) we supervise each pair
        of matched ground-truth / prediction (supervise class and mask)

        Args:
            config (`Mask2FormerConfig`):
                The configuration for Mask2Former model also containing loss calculation specific parameters.
        """
        super().__init__()
        requires_backends(self, ["scipy"])

        # Weight to apply to the null class
        self.eos_coef = config.no_object_weight # 0.1

        # matcher
        self.matcher = Mask2FormerHungarianMatcher(config=config)
        

    def _max_by_axis(self, sizes: List[List[int]]) -> List[int]:
        """
        Computes the element-wise maximum across all lists provided in sizes.

        Parameters:
            - sizes (List[List[int]]): A list of lists, where each sublist represents dimensions (e.g., height, width) of a tensor or array.
                                       All sublists should have the same length, representing the same number of dimensions but possibly different sizes.

        Returns:
            - List[int]: A list of integers representing the maximum size found for each dimension across all input lists. The length of this list 
                         corresponds to the number of dimensions being compared.
        """
        
        maxes = sizes[0]
        for sublist in sizes[1:]:
            for index, item in enumerate(sublist):
                maxes[index] = max(maxes[index], item)

        return maxes


    def _pad_images_to_max_in_batch(self, tensors: List[Tensor]) -> Tuple[Tensor, Tensor]:
        """
        Pads all images in the batch to the maximum size found among them.

        Parameters:
            - tensors (List[torch.Tensor]): A list of image tensors. Each tensor has a shape of 
                                            `(channels, height, width)`. The number of channels should be consistent across all tensors,
                                            but heights and widths can vary.

        Returns:
            - Tuple[torch.Tensor, torch.Tensor]: A tuple containing two elements:
                1. `padded_tensors` (torch.Tensor): A tensor of shape `(batch_size, channels, max_height, max_width)` where each image 
                    has been padded to match the largest height and width in the batch.
                2. `padding_masks` (torch.Tensor): A boolean tensor of shape `(batch_size, max_height, max_width)` indicating which parts 
                    of the padded tensors are padding. True values indicate padding regions, while False values indicate actual image data.

        Note:
            This function first computes the maximum size for each dimension (height and width) across all images in the batch. Then it 
            creates new tensors filled with zeros (for images) and ones (for masks), and copies the original images into these new tensors.
            The mask tensor is used to indicate which areas of the padded tensors contain actual image data versus padding.
        """
        
        # get the maximum size in the batch
        max_size = self._max_by_axis([list(tensor.shape) for tensor in tensors])
        
        # compute final size
        batch_shape = [len(tensors)] + max_size
        batch_size, _, height, width = batch_shape
        dtype = tensors[0].dtype
        device = tensors[0].device
        padded_tensors = torch.zeros(batch_shape, dtype=dtype, device=device)
        padding_masks = torch.ones((batch_size, height, width), dtype=torch.bool, device=device)
        # pad the tensors to the size of the biggest one
        for tensor, padded_tensor, padding_mask in zip(tensors, padded_tensors, padding_masks):
            padded_tensor[: tensor.shape[0], : tensor.shape[1], : tensor.shape[2]].copy_(tensor)
            padding_mask[: tensor.shape[1], : tensor.shape[2]] = False

        return padded_tensors, padding_masks


    def _pad_bboxes_to_max_in_batch(self, tensors: List[Tensor]) -> Tuple[Tensor, Tensor]:
        """
        Pads all bounding boxes in the batch to the maximum size found among them.

        Parameters:
            - tensors (List[torch.Tensor]): A list of bounding box tensors. Each tensor has a shape of `(num_boxes, dimensions)`,
                                            where `num_boxes` is the number of bounding boxes and `dimensions` is the number of 
                                            dimensions for each bounding box (typically 4 for coordinates). The number of dimensions 
                                            should be consistent across all tensors, but the number of boxes can vary.

        Returns:
            - Tuple[torch.Tensor, torch.Tensor]: A tuple containing two elements:
                1. `padded_tensors` (torch.Tensor): A tensor of shape `(batch_size, max_num_boxes, dimensions)` where each set of 
                    bounding boxes has been padded to match the largest number of boxes in the batch.
                2. `padding_masks` (torch.Tensor): A boolean tensor of shape `(batch_size, max_num_boxes)` indicating which parts of 
                    the padded tensors are padding. True values indicate padding regions, while False values indicate actual bounding box data.

        Note:
            This function first computes the maximum number of bounding boxes across all sets in the batch. Then it creates new tensors 
            filled with zeros (for bounding boxes) and ones (for masks), and copies the original bounding boxes into these new tensors. 
            The mask tensor is used to indicate which areas of the padded tensors contain actual bounding box data versus padding.
        """
        
        # get the maximum size in the batch
        max_size = self._max_by_axis([list(tensor.shape) for tensor in tensors])
        
        # compute final size
        batch_shape = [len(tensors)] + max_size
        batch_size, _, dim = batch_shape
        dtype = tensors[0].dtype
        device = tensors[0].device
        padded_tensors = torch.zeros(batch_shape, dtype=dtype, device=device)
        padding_masks  = torch.ones((batch_size, dim), dtype=torch.bool, device=device)
        # pad the tensors to the size of the biggest one
        for tensor, padded_tensor, padding_mask in zip(tensors, padded_tensors, padding_masks):
            padded_tensor[: tensor.shape[0], : tensor.shape[1]].copy_(tensor)
            padding_mask[: tensor.shape[1]] = False

        return padded_tensors, padding_masks


    def loss_labels(
        self, 
        class_names: List[list[str]],
        instance_labels: List[Tensor], 
        instance_logits: Tensor, 
        indices: Tuple[np.array], 
    ) -> Dict[str, Tensor]:
        """
        Compute the losses related to the labels using cross entropy.

        Parameters:
            - class_names (List[List[str]]): A list of lists containing class names for each batch item. Each sublist corresponds to one image's class names.
            - instance_labels (List[torch.Tensor]): A list of tensors where each tensor contains class labels for each instance in the corresponding image. 
                The shape of each tensor is `(num_instances,)`.
            - instance_logits (torch.Tensor): A tensor of shape `(batch_size, num_queries, num_labels)` representing the predicted logits for each query and each label.
            - indices (Tuple[np.array]): The indices computed by the Hungarian matcher. It is a tuple of two arrays, where the first array contains indices of 
                predictions and the second array contains indices of ground truth labels. These indices indicate which prediction matches which ground truth label.

        Returns:
            - Dict[str, torch.Tensor]: A dictionary containing the computed loss under the key 'loss_class'. This loss is computed using cross entropy on the predicted and ground truth labels.
            
        Note:
            This function iterates over each item in the batch, computes the target classes based on the matched indices from the Hungarian matcher, applies a weight 
            to account for the end-of-sequence (EOS) token if applicable, and then computes the cross-entropy loss. Finally, it averages the loss over the batch size.
        """
        
        batch_size, num_queries, max_num_labels = instance_logits.shape

        loss_class = 0
        for i in range(batch_size):
            num_labels = len(class_names[i])
            target_classes = torch.full((num_queries,), fill_value=num_labels-1, dtype=torch.int64, device=instance_logits.device)
            target_classes[indices[i][0]] = instance_labels[i][indices[i][1]]

            label_weight = torch.ones(num_labels).to(instance_logits.device)
            label_weight[-1] = self.eos_coef
            
            loss_ce = nn.functional.cross_entropy(instance_logits[i][:,:num_labels], target_classes, label_weight, reduction="mean")
            loss_class = loss_class + loss_ce
            
        loss_class = loss_class / batch_size
        losses = {"loss_class": loss_class}
        #print(losses)
        
        return losses
    

    def loss_instacne_masks(
        self,
        true_instance_masks:  List[Tensor],
        true_instance_bboxes: List[Tensor],
        pred_instance_masks:  Tensor,
        pred_instance_bboxes: Tensor,
        indices: Tuple[np.array],
    ) -> Dict[str, Tensor]:
        """
        Compute the losses related to instance masks and bounding boxes.

        Parameters:
            - true_instance_masks (List[torch.Tensor]): A list of tensors where each tensor contains ground truth masks for each instance in the 
                corresponding image. Each mask tensor has a shape of `(num_instances, height, width)`.
            - true_instance_bboxes (List[torch.Tensor]): A list of tensors where each tensor contains ground truth bounding boxes for each instance 
                in the corresponding image. Each bounding box tensor has a shape of `(num_instances, 4)`.
            - pred_instance_masks (torch.Tensor): A tensor of shape `(batch_size, num_queries, height, width)` representing the predicted instance masks.
            - pred_instance_bboxes (torch.Tensor): A tensor of shape `(batch_size, num_queries, 4)` representing the predicted bounding boxes.
            - indices (Tuple[np.array]): The indices computed by the Hungarian matcher. It is a tuple of two arrays, where the first array contains indices of 
                predictions and the second array contains indices of ground truth labels. These indices indicate which prediction matches which ground truth label.

        Returns:
            - Dict[str, torch.Tensor]: A dictionary containing the computed losses with the following keys:
                - **loss_sml1** -- The smooth L1 loss computed between predicted and ground truth bounding boxes.
                - **loss_diou** -- The distance IoU loss computed between predicted and ground truth bounding boxes.
                - **loss_dice** -- The Dice loss computed between predicted and ground truth masks.
                - **loss_focal** -- The focal loss computed between predicted and ground truth masks, scaled by a factor of 10.

        Note:
            This function computes losses for both bounding boxes and masks based on the matched indices from the Hungarian matcher. 
            For bounding boxes, it calculates smooth L1 loss and distance IoU loss. For masks, it samples points from the predicted 
            and ground truth masks and then calculates Dice loss and focal loss.
        """
        
        losses = {}
        if pred_instance_bboxes is not None:
            pred_bboxes = [bboxes[ids[0]] for ids, bboxes in zip(indices, pred_instance_bboxes)]
            true_bboxes = [bboxes[ids[1]] for ids, bboxes in zip(indices, true_instance_bboxes)]
            pred_bboxes = torch.cat(pred_bboxes, dim=0)
            true_bboxes = torch.cat(true_bboxes, dim=0)

            losses["loss_sml1"] = smooth_l1_loss(pred_bboxes/100, true_bboxes/100, reduction="mean")
            losses["loss_diou"] = distance_box_iou_loss(pred_bboxes, true_bboxes, reduction="mean")

        if pred_instance_masks is not None:
            pred_masks = [masks[ids[0]] for ids, masks in zip(indices, pred_instance_masks)]
            true_masks = [masks[ids[1]] for ids, masks in zip(indices, true_instance_masks)]
            pred_masks = torch.cat(pred_masks, dim=0)[:, None]
            true_masks = torch.cat(true_masks, dim=0)[:, None]
            
            point_coordinates = sample_grid_points(pred_masks.shape[2], pred_masks.shape[3], is_offset=True)
            point_coordinates = point_coordinates.expand(pred_masks.size(0), point_coordinates.shape[1], point_coordinates.shape[2]).to(pred_masks.device)
            point_labels = sample_point(true_masks, point_coordinates, align_corners=False).squeeze(1)
            point_logits = sample_point(pred_masks, point_coordinates, align_corners=False).squeeze(1)
            
            losses["loss_dice"]  = sigmoid_dice_loss(point_logits, point_labels, reduction='mean')
            losses["loss_focal"] = sigmoid_focal_loss(point_logits, point_labels, reduction='mean') * 10
            
        #print(losses)
        return losses


    def loss_semantic_masks(
        self,
        true_semantic_masks: List[Tensor],
        pred_semantic_masks: Tensor,
        indices: Tuple[np.array],
    ) -> Dict[str, Tensor]:
        """
        Compute the losses related to semantic masks.

        Parameters:
            - true_semantic_masks (List[torch.Tensor]): A list of tensors where each tensor contains ground truth semantic masks for each image. 
                Each mask tensor has a shape of `(num_classes, height, width)`.
            - pred_semantic_masks (torch.Tensor): A tensor of shape `(batch_size, num_classes, height, width)` representing the predicted semantic masks.
            - indices (Tuple[np.array]): The indices computed by the Hungarian matcher. It is a tuple of two arrays, where the first array contains indices 
                of predictions and the second array contains indices of ground truth labels. These indices indicate which prediction matches which ground truth label.

        Returns:
            - Dict[str, torch.Tensor]: A dictionary containing the computed losses with the following keys:
                - **loss_dice_semantic** -- The Dice loss computed between predicted and ground truth semantic masks.
                - **loss_focal_semantic** -- The focal loss computed between predicted and ground truth semantic masks, scaled by a factor of 10.

        Note:
            This function computes losses for semantic masks based on the matched indices from the Hungarian matcher. 
            It samples points from the predicted and ground truth masks and then calculates Dice loss and focal loss. 
            The focal loss is scaled by a factor of 10 to give it more weight during training.
        """
        
        losses = {}
        if pred_semantic_masks is not None:
            pred_masks = [masks[ids[0]] for ids, masks in zip(indices, pred_semantic_masks)]
            true_masks = [masks[ids[1]] for ids, masks in zip(indices, true_semantic_masks)]
            pred_masks = torch.cat(pred_masks, dim=0)[:, None]
            true_masks = torch.cat(true_masks, dim=0)[:, None]
        
            point_coordinates = sample_grid_points(pred_masks.shape[2], pred_masks.shape[3], is_offset=True)
            point_coordinates = point_coordinates.expand(pred_masks.shape[0], point_coordinates.shape[1], point_coordinates.shape[2]).to(pred_masks.device)
            point_labels = sample_point(true_masks, point_coordinates, align_corners=False).squeeze(1)
            point_logits = sample_point(pred_masks, point_coordinates, align_corners=False).squeeze(1)
            
            losses["loss_dice_semantic"]  = sigmoid_dice_loss(point_logits, point_labels, reduction='mean')
            losses["loss_focal_semantic"] = sigmoid_focal_loss(point_logits, point_labels, reduction='mean') * 10

        return losses
    

    def forward(
        self,
        class_names: List[list[str]],
        true_instance_masks: Optional[List[Tensor]] = None,
        true_instance_bboxes: Optional[List[Tensor]] = None,
        true_instance_labels: Optional[List[Tensor]] = None,
        true_semantic_masks: Optional[List[Tensor]] = None,
        pred_instance_masks: Optional[Tensor] = None,
        pred_instance_bboxes: Optional[Tensor] = None,
        pred_instance_labels: Optional[Tensor] = None,
        pred_semantic_masks: Optional[Tensor] = None,
        auxiliary_predictions: Optional[List[Dict]] = None,
    ) -> Dict[str, Tensor]:
        """
        This performs the loss computation.

        Parameters:
            - class_names (List[List[str]]): 
                A list of lists containing class names for each batch item. Each sublist corresponds to one image's class names.
            - true_instance_masks (Optional[List[torch.Tensor]]): 
                List of ground truth instance masks for each image in the batch. Each mask tensor has a shape of `(num_instances, height, width)`.
            - true_instance_bboxes (Optional[List[torch.Tensor]]): 
                List of ground truth bounding boxes for each image in the batch. Each bounding box tensor has a shape of `(num_instances, 4)`.
            - true_instance_labels (Optional[List[torch.Tensor]]): 
                List of ground truth labels for each instance in the batch. Each label tensor has a shape of `(num_instances,)`.
            - true_semantic_masks (Optional[List[torch.Tensor]]): 
                List of ground truth semantic masks for each image in the batch. Each mask tensor has a shape of `(height, width)` or `(num_classes, height, width)`.
            - pred_instance_masks (Optional[torch.Tensor]): 
                Tensor of predicted instance masks for all images in the batch. Shape is `(batch_size, num_queries, height, width)`.
            - pred_instance_bboxes (Optional[torch.Tensor]): 
                Tensor of predicted bounding boxes for all images in the batch. Shape is `(batch_size, num_queries, 4)`.
            - pred_instance_labels (Optional[torch.Tensor]): 
                Tensor of predicted labels for all instances in the batch. Shape is `(batch_size, num_queries)`.
            - pred_semantic_masks (Optional[torch.Tensor]): 
                Tensor of predicted semantic masks for all images in the batch. Shape is `(batch_size, num_classes, height, width)`.
            - auxiliary_predictions (Optional[List[Dict]], optional): 
                List of dictionaries containing logits from the inner layers of the Mask2FormerMaskedAttentionDecoder if auxiliary losses are enabled.

        Returns:
            - Dict[str, torch.Tensor]: A dictionary containing the computed losses with keys including:
                - **loss_class** -- The loss computed using cross entropy on the predicted and ground truth labels.
                - **loss_mask** -- The loss computed using sigmoid cross-entropy loss on the predicted and ground truth masks.
                - **loss_dice** -- The loss computed using dice loss on the predicted and ground truth masks.
                If auxiliary losses are enabled, the dictionary contains additional losses for each auxiliary prediction.

        Note:
            This function computes the losses for both instance-level and semantic segmentation tasks. It uses the Hungarian matcher to find the matching between 
            predictions and ground truths. Additionally, it handles auxiliary predictions by recursively calling itself with outputs from intermediate layers.
        """
        
        # retrieve the matching between the outputs of the last layer and the labels
        instance_indices = self.matcher(class_names, pred_instance_masks, pred_instance_bboxes, pred_instance_labels, true_instance_masks, true_instance_bboxes, true_instance_labels)
        semantic_indices = [(torch.arange(item.size(0)), torch.arange(item.size(0))) for item in true_semantic_masks]

        # get all the losses
        losses: Dict[str, Tensor] = {
            **self.loss_instacne_masks(true_instance_masks, true_instance_bboxes, pred_instance_masks, pred_instance_bboxes, instance_indices),
            **self.loss_semantic_masks(true_semantic_masks, pred_semantic_masks, semantic_indices),
            **self.loss_labels(class_names, true_instance_labels, pred_instance_labels, instance_indices),
        }
        
        # in case of auxiliary losses, we repeat this process with the output of each intermediate layer.
        if auxiliary_predictions is not None:
            for idx, aux_outputs in enumerate(auxiliary_predictions):
                pred_instance_masks  = aux_outputs["pred_instance_masks"]
                pred_instance_bboxes = aux_outputs["pred_instance_bboxes"]
                pred_instance_labels = aux_outputs["pred_instance_labels"]
                pred_semantic_masks  = aux_outputs["pred_semantic_masks"]
                    
                loss_dict = self.forward(
                    class_names = class_names,
                    true_instance_masks = true_instance_masks,
                    true_instance_bboxes = true_instance_bboxes,
                    true_instance_labels = true_instance_labels,
                    true_semantic_masks = true_semantic_masks,
                    pred_instance_masks = pred_instance_masks,
                    pred_instance_bboxes = pred_instance_bboxes,
                    pred_instance_labels = pred_instance_labels,
                    pred_semantic_masks = pred_semantic_masks,
                    auxiliary_predictions = None,
                )

                loss_dict = {f"{key}_{idx}": value for key, value in loss_dict.items()}
                losses.update(loss_dict)
                
        return losses


# Copied from transformers.models.deformable_detr.modeling_deformable_detr.multi_scale_deformable_attention
def multi_scale_deformable_attention(
    value: Tensor, value_spatial_shapes: Tensor, sampling_locations: Tensor, attention_weights: Tensor
) -> Tensor:
    """
    Performs multi-scale deformable attention mechanism.

    Parameters:
        - value (torch.Tensor): 
            A tensor of shape `(batch_size, num_values, num_heads, hidden_dim)` representing the value embeddings.
            Each value embedding is used to compute the output for each query based on the attention weights and sampling locations.
        - value_spatial_shapes (torch.Tensor): 
            A tensor of shape `(num_levels, 2)` indicating the spatial shapes `(height, width)` of the values at different levels.
            This allows handling features from multiple scales or resolutions.
        - sampling_locations (torch.Tensor): 
            A tensor of shape `(batch_size, num_queries, num_heads, num_levels, num_points, 2)` containing the normalized sampling locations.
            These locations specify where to sample the value embeddings for each query.
        - attention_weights (torch.Tensor): 
            A tensor of shape `(batch_size, num_queries, num_heads, num_levels, num_points)` representing the attention weights.
            These weights determine the importance of each sampled point when aggregating the final output.

    Returns:
        - torch.Tensor: A tensor of shape `(batch_size, num_queries, num_heads * hidden_dim)` representing the aggregated output after applying the multi-scale deformable attention mechanism.

    Note:
        This function implements a multi-scale deformable attention mechanism that can handle inputs from multiple feature levels with varying spatial resolutions. 
        It samples the value embeddings according to the provided sampling locations and aggregates them using the attention weights.
    """
    
    batch_size, _, num_heads, hidden_dim = value.shape
    _, num_queries, num_heads, num_levels, num_points, _ = sampling_locations.shape
    value_list = value.split([height.item() * width.item() for height, width in value_spatial_shapes], dim=1)
    sampling_grids = 2 * sampling_locations - 1
    sampling_value_list = []
    
    for level_id, (height, width) in enumerate(value_spatial_shapes):
        # batch_size, height*width, num_heads, hidden_dim
        # -> batch_size, height*width, num_heads*hidden_dim
        # -> batch_size, num_heads*hidden_dim, height*width
        # -> batch_size*num_heads, hidden_dim, height, width
        value_l_ = (
            value_list[level_id].flatten(2).transpose(1, 2).reshape(batch_size * num_heads, hidden_dim, height, width)
        )
        # batch_size, num_queries, num_heads, num_points, 2
        # -> batch_size, num_heads, num_queries, num_points, 2
        # -> batch_size*num_heads, num_queries, num_points, 2
        sampling_grid_l_ = sampling_grids[:, :, :, level_id].transpose(1, 2).flatten(0, 1)
        # batch_size*num_heads, hidden_dim, num_queries, num_points
        sampling_value_l_ = nn.functional.grid_sample(
            value_l_, sampling_grid_l_, mode="bilinear", padding_mode="zeros", align_corners=False
        )
        sampling_value_list.append(sampling_value_l_)
    # (batch_size, num_queries, num_heads, num_levels, num_points)
    # -> (batch_size, num_heads, num_queries, num_levels, num_points)
    # -> (batch_size, num_heads, 1, num_queries, num_levels*num_points)
    attention_weights = attention_weights.transpose(1, 2).reshape(
        batch_size * num_heads, 1, num_queries, num_levels * num_points
    )
    output = (
        (torch.stack(sampling_value_list, dim=-2).flatten(-2) * attention_weights)
        .sum(-1)
        .view(batch_size, num_heads * hidden_dim, num_queries)
    )
    
    return output.transpose(1, 2).contiguous()


# Copied from transformers.models.maskformer.modeling_maskformer.MaskFormerSinePositionEmbedding with MaskFormer->Mask2Former
class Mask2FormerSinePositionEmbedding(nn.Module):
    """
    This is a more standard version of the position embedding, very similar to the one used by the Attention is all you
    need paper, generalized to work on images.
    """

    def __init__(
        self, dim: int = 64, temperature: int = 10000, scale: Optional[float] = None
    ):
        """
        Initialize the Mask2FormerSinePositionEmbedding module.

        Parameters:
            - dim (int): 
                The dimensionality of the output embeddings. Default is 64.
            - temperature (int): 
                A constant that controls the frequency of the sine and cosine functions used for positional encoding. Default is 10000.
            - scale (Optional[float]): 
                An optional scaling factor for normalization. If provided, it should be combined with normalization being enabled. Default is None, which sets the scale to 2 * pi.
        """
        
        super().__init__()
        if scale is not None and normalize is False:
            raise ValueError("normalize should be True if scale is passed")
        self.dim = dim
        self.temperature = temperature
        self.scale = 2 * math.pi if scale is None else scale

    def forward(self, x: Tensor, mask: Optional[Tensor] = None, normalize: bool = False,) -> Tensor:
        """
        Forward pass of the Mask2FormerSinePositionEmbedding module.

        Parameters:
            - x (torch.Tensor): 
                Input tensor of shape `(batch_size, channels, height, width)` to which position embeddings will be added.
            - mask (Optional[torch.Tensor]): 
                An optional binary mask tensor of shape `(batch_size, height, width)`. Positions corresponding to mask values of 1 are not masked, while those with 0 are masked. 
                If not provided, no positions are masked.
            - normalize (bool): 
                A flag indicating whether to normalize the position embeddings. If True, the embeddings are normalized by the maximum spatial dimension. Default is False.

        Returns:
            - torch.Tensor: 
                A tensor of shape `(batch_size, C, H, W)` representing the input tensor `x` with added sine-based positional embeddings.

        Note:
            This function computes the sine and cosine positional encodings for each position in the input tensor `x` based on its spatial dimensions (height and width). 
            These encodings are then concatenated and permuted to match the expected output shape.
        """
        
        if mask is None:
            mask = torch.zeros((x.size(0), x.size(2), x.size(3)), device=x.device, dtype=torch.bool)
        not_mask = (~mask).to(x.dtype)
        y_embed = not_mask.cumsum(1)
        x_embed = not_mask.cumsum(2)
        
        if normalize:
            eps = 1e-6
            y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale
            x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale

        dim_t = torch.arange(self.dim//2, dtype=torch.int64, device=x.device).type_as(x)
        dim_t = self.temperature ** (2 * torch.div(dim_t, 2, rounding_mode="floor") / (self.dim//2))

        pos_x = x_embed[:, :, :, None] / dim_t
        pos_y = y_embed[:, :, :, None] / dim_t
        pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)
        pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)
        position_embed = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)

        return position_embed


# Learnable 2d Position Embedding
class LearnablePositionEmbedding2d(nn.Module):
    """
    A class for generating learnable 2D position embeddings, which can be added to vision features.
    """
    
    def __init__(self, hei:int=256, wid:int=256, dim=256):
        """
        Initialize the LearnablePositionEmbedding2d module.

        Parameters:
            - hei (int): 
                Height of the position embedding grid. Default is 256.
            - wid (int): 
                Width of the position embedding grid. Default is 256.
            - dim (int): 
                Dimensionality of the output embeddings. This should match the channel dimension of the vision feature input in the forward method. Default is 256.
        """
        
        super().__init__()
        self.hei = hei
        self.wid = wid
        self.dim = dim
        self.position_embed_x = nn.Embedding(self.wid, self.dim//2)
        self.position_embed_y = nn.Embedding(self.hei, self.dim//2)
        self.norm = nn.LayerNorm(self.dim)

    def forward(self, vision_feature, img_bboxes=None, normalize: bool = False,):
        """
        Forward pass of the LearnablePositionEmbedding2d module.

        Parameters:
            - vision_feature (torch.Tensor): 
                Input tensor of shape `(batch_size, channels, height, width)` representing the vision features to which position embeddings will be added.
            - img_bboxes (Optional[torch.Tensor]): 
                An optional tensor of shape `(batch_size, 4)` containing bounding boxes `[left, top, right, bottom]` for each image in the batch. If provided, 
                position embeddings are interpolated to fit within these bounding boxes. Default is None.
            - normalize (bool): 
                A flag indicating whether to apply layer normalization on the position embeddings before adding them to the vision features. Default is False.

        Returns:
            - torch.Tensor: 
                A tensor of shape `(batch_size, channels, height, width)` representing the input `vision_feature` with added learnable position embeddings.

        Note:
            This function generates learnable position embeddings for both x and y dimensions separately and then concatenates them along the channel dimension. 
            If bounding boxes are provided, it interpolates the position embeddings to fit within those bounding boxes; otherwise, it interpolates them to match 
            the size of the input vision features.
        """
        
        N, C, H, W = vision_feature.size()
        
        position_embed_x = self.position_embed_x.weight[None, None, :, :].expand(N, self.hei, self.wid, self.dim//2)
        position_embed_y = self.position_embed_y.weight[None, :, None, :].expand(N, self.hei, self.wid, self.dim//2)
        position_embed = torch.cat((position_embed_x, position_embed_y), dim=-1).permute(0,3,1,2)
        
        if normalize:
            position_embed = self.norm(position_embed)

        if img_bboxes is not None:
            vision_posembs = torch.zeros((N, self.dim, H, W)).to(position_embed)
            for im, img_bbox in enumerate(img_bboxes):
                l, t, r, b = img_bbox.long()
                vision_posembs[im:im+1, :, t:b, l:r] = nn.functional.interpolate(position_embed[im:im+1], size=(b-t, r-l), mode="bilinear", align_corners=False)
        else:
            vision_posembs = nn.functional.interpolate(position_embed, size=(H,W), mode="bilinear", align_corners=False)
        
        return vision_posembs


# Modified from transformers.models.detr.modeling_deformable_detr.DeformableDetrMultiscaleDeformableAttention
class Mask2FormerPixelDecoderEncoderMultiscaleDeformableAttention(nn.Module):
    """
    Multiscale deformable attention as proposed in Deformable DETR.
    """

    def __init__(self, embed_dim: int, num_heads: int, n_levels: int, n_points: int):
        """
        Initialize the Mask2FormerPixelDecoderEncoderMultiscaleDeformableAttention module.

        Parameters:
            - embed_dim (int): 
                The dimensionality of the embeddings. Also referred to as d_model. It must be divisible by num_heads.
            - num_heads (int): 
                Number of attention heads. embed_dim should be divisible by this value.
            - n_levels (int): 
                Number of feature levels used in the multi-scale deformable attention mechanism.
            - n_points (int): 
                Number of sampling points per attention head per feature level.
        """
        
        super().__init__()
        if embed_dim % num_heads != 0:
            raise ValueError(
                f"embed_dim (d_model) must be divisible by num_heads, but got {embed_dim} and {num_heads}"
            )
        dim_per_head = embed_dim // num_heads
        # check if dim_per_head is power of 2
        if not ((dim_per_head & (dim_per_head - 1) == 0) and dim_per_head != 0):
            warnings.warn(
                "You'd better set embed_dim (d_model) in DeformableDetrMultiscaleDeformableAttention to make the"
                " dimension of each attention head a power of 2 which is more efficient in the authors' CUDA"
                " implementation."
            )

        self.im2col_step = 128

        self.d_model = embed_dim
        self.n_levels = n_levels
        self.n_heads = num_heads
        self.n_points = n_points

        self.sampling_offsets = nn.Linear(embed_dim, num_heads * n_levels * n_points * 2)
        self.attention_weights = nn.Linear(embed_dim, num_heads * n_levels * n_points)
        self.value_proj = nn.Linear(embed_dim, embed_dim)
        self.output_proj = nn.Linear(embed_dim, embed_dim)

    def with_pos_embed(self, tensor: Tensor, position_embeddings: Optional[Tensor]):
        return tensor if position_embeddings is None else tensor + position_embeddings

    def forward(
        self,
        hidden_states: Tensor,
        attention_mask: Optional[Tensor] = None,
        encoder_hidden_states=None,
        encoder_attention_mask=None,
        position_embeddings: Optional[Tensor] = None,
        reference_points=None,
        spatial_shapes=None,
        level_start_index=None,
        output_attentions: bool = False,
    ):
        """
        Forward pass of the Mask2FormerPixelDecoderEncoderMultiscaleDeformableAttention module.

        Parameters:
            - hidden_states (torch.Tensor): 
                Input tensor of shape `(batch_size, num_queries, embed_dim)` representing the queries for the attention mechanism.
            - attention_mask (Optional[torch.Tensor]): 
                An optional mask tensor of shape `(batch_size, num_queries)` to mask out certain queries. Default is None.
            - encoder_hidden_states (Optional[torch.Tensor]): 
                An optional tensor of shape `(batch_size, sequence_length, embed_dim)` representing the encoder's hidden states. Default is None.
            - encoder_attention_mask (Optional[torch.Tensor]): 
                An optional mask tensor of shape `(batch_size, sequence_length)` to mask out certain positions in the encoder hidden states. Default is None.
            - position_embeddings (Optional[torch.Tensor]): 
                An optional tensor of shape `(batch_size, num_queries, embed_dim)` containing positional embeddings to be added to the hidden states before processing. Default is None.
            - reference_points (torch.Tensor): 
                Tensor of shape `(batch_size, num_queries, num_levels, 2 or 4)` representing reference points for sampling offsets.
            - spatial_shapes (torch.Tensor): 
                Tensor of shape `(n_levels, 2)` indicating the spatial shapes `(height, width)` of features at different levels.
            - level_start_index (Optional[torch.Tensor]): 
                An optional tensor of shape `(n_levels,)` indicating the start index of each level in the sequence. Default is None.
            - output_attentions (bool): 
                A flag indicating whether to return the attention weights along with the output. Default is False.

        Returns:
            - Tuple[torch.Tensor, Optional[torch.Tensor]]: 
                A tuple containing the output tensor of shape `(batch_size, num_queries, embed_dim)` and optionally the attention weights if `output_attentions` is True.
        """
        
        # add position embeddings to the hidden states before projecting to queries and keys
        if position_embeddings is not None:
            hidden_states = self.with_pos_embed(hidden_states, position_embeddings)

        batch_size, num_queries, _ = hidden_states.shape
        batch_size, sequence_length, _ = encoder_hidden_states.shape
        if (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() != sequence_length:
            raise ValueError(
                "Make sure to align the spatial shapes with the sequence length of the encoder hidden states"
            )

        value = self.value_proj(encoder_hidden_states)
        if attention_mask is not None:
            # we invert the attention_mask
            value = value.masked_fill(attention_mask[..., None], float(0))
        value = value.view(batch_size, sequence_length, self.n_heads, self.d_model // self.n_heads)
        sampling_offsets = self.sampling_offsets(hidden_states).view(
            batch_size, num_queries, self.n_heads, self.n_levels, self.n_points, 2
        )
        attention_weights = self.attention_weights(hidden_states).view(
            batch_size, num_queries, self.n_heads, self.n_levels * self.n_points
        )
        attention_weights = nn.functional.softmax(attention_weights, -1).view(
            batch_size, num_queries, self.n_heads, self.n_levels, self.n_points
        )
        # batch_size, num_queries, n_heads, n_levels, n_points, 2
        if reference_points.shape[-1] == 2:
            offset_normalizer = torch.stack([spatial_shapes[..., 1], spatial_shapes[..., 0]], -1)
            sampling_locations = (
                reference_points[:, :, None, :, None, :]
                + sampling_offsets / offset_normalizer[None, None, None, :, None, :]
            )
        elif reference_points.shape[-1] == 4:
            sampling_locations = (
                reference_points[:, :, None, :, None, :2]
                + sampling_offsets / self.n_points * reference_points[:, :, None, :, None, 2:] * 0.5
            )
        else:
            raise ValueError(f"Last dim of reference_points must be 2 or 4, but got {reference_points.shape[-1]}")

        output = multi_scale_deformable_attention(value, spatial_shapes, sampling_locations, attention_weights)
        output = self.output_proj(output)
        
        return output, attention_weights


# Mask2FormerPixelDecoderEncoderLayer
class Mask2FormerPixelDecoderEncoderLayer(nn.Module):
    def __init__(self, config: Mask2FormerConfig):
        """
        Initialize the Mask2FormerPixelDecoderEncoderLayer module.

        Parameters:
            - config (Mask2FormerConfig): 
                Configuration object containing model parameters such as feature_size, num_attention_heads, dropout, encoder_feedforward_dim.
        """
        
        super().__init__()
        self.embed_dim = config.feature_size
        self.self_attn = Mask2FormerPixelDecoderEncoderMultiscaleDeformableAttention(
            embed_dim=self.embed_dim,
            num_heads=config.num_attention_heads,
            n_levels=3,
            n_points=4,
        )

        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)
        self.dropout = config.dropout
        self.activation_fn = nn.functional.silu
        self.activation_dropout = config.dropout
        self.fc1 = nn.Linear(self.embed_dim, config.encoder_feedforward_dim)
        self.fc2 = nn.Linear(config.encoder_feedforward_dim, self.embed_dim)
        self.final_layer_norm = nn.LayerNorm(self.embed_dim)

    def forward(
        self,
        hidden_states: Tensor,
        attention_mask: Tensor,
        position_embeddings: Tensor = None,
        reference_points=None,
        spatial_shapes=None,
        level_start_index=None,
        output_attentions: bool = False,
    ):
        """
        Forward pass of the Mask2FormerPixelDecoderEncoderLayer module.

        Parameters:
            - hidden_states (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):
                Input tensor to the layer. This is usually a batch of token embeddings or intermediate hidden states.
            - attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):
                Attention mask indicating which tokens should be attended to. Typically, this is used to mask padding tokens.
            - position_embeddings (`torch.FloatTensor`, *optional*):
                Position embeddings that are added to the input hidden states before processing. Default is None.
            - reference_points (`torch.FloatTensor`, *optional*):
                Reference points for deformable attention mechanism. These are used to guide where the network should attend. Default is None.
            - spatial_shapes (`torch.LongTensor`, *optional*):
                Tensor specifying the spatial shapes of feature maps at different levels. Used in multi-scale deformable attention. Default is None.
            - level_start_index (`torch.LongTensor`, *optional*):
                Start indices for each level of the multi-scale features. Useful when dealing with features from multiple scales. Default is None.
            - output_attentions (`bool`, *optional*):
                If set to True, returns the attention weights along with the output. Useful for debugging or visualization purposes. Default is False.

        Returns:
            - Tuple[torch.Tensor, Optional[torch.Tensor]]:
                A tuple containing the updated hidden states and optionally the attention weights if `output_attentions` is True.
        """
        
        residual = hidden_states

        # Apply Multi-scale Deformable Attention Module on the multi-scale feature maps.
        hidden_states, attn_weights = self.self_attn(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            encoder_hidden_states=hidden_states,
            encoder_attention_mask=attention_mask,
            position_embeddings=position_embeddings,
            reference_points=reference_points,
            spatial_shapes=spatial_shapes,
            level_start_index=level_start_index,
            output_attentions=output_attentions,
        )

        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)
        hidden_states = residual + hidden_states
        hidden_states = self.self_attn_layer_norm(hidden_states)

        residual = hidden_states
        hidden_states = self.activation_fn(self.fc1(hidden_states))
        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)

        hidden_states = self.fc2(hidden_states)
        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)

        hidden_states = residual + hidden_states
        hidden_states = self.final_layer_norm(hidden_states)

        if self.training:
            if torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any():
                clamp_value = torch.finfo(hidden_states.dtype).max - 1000
                hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)

        outputs = (hidden_states,)

        if output_attentions:
            outputs += (attn_weights.transpose(1, 0),)

        return outputs


# Modified from from transformers.models.detr.modeling_deformable_detr.DeformableDetrEncoder with DeformableDetrEncoder->Mask2FormerPixelDecoderEncoderOnly
class Mask2FormerPixelDecoderEncoderOnly(nn.Module):
    """
    Transformer encoder consisting of *config.encoder_layers* deformable attention layers. Each layer is a
    [`Mask2FormerPixelDecoderEncoderLayer`]. The encoder updates the flattened multi-scale feature maps through
    multiple deformable attention layers.

    Args:
        config (Mask2FormerConfig): Configuration object containing model parameters.
    """

    def __init__(self, config: Mask2FormerConfig):
        super().__init__()

        self.config = config
        self.dropout = config.dropout
        self.layers = nn.ModuleList(
            [Mask2FormerPixelDecoderEncoderLayer(config) for _ in range(config.encoder_layers)]
        )

    @staticmethod
    def get_reference_points(spatial_shapes, valid_ratios, device):
        """
        Get reference points for each feature map. Used in decoder.

        Parameters:
            spatial_shapes (`torch.LongTensor`):
                Spatial shapes of each feature map, has shape of `(num_levels, 2)`.
            valid_ratios (`torch.FloatTensor`):
                Valid ratios of each feature map, has shape of `(batch_size, num_levels, 2)`.
            device (`torch.device`):
                Device on which to create the tensors.

        Returns:
            `torch.FloatTensor` of shape `(batch_size, num_queries, num_levels, 2)`
        """
        
        reference_points_list = []
        for lvl, (height, width) in enumerate(spatial_shapes):
            ref_y, ref_x = torch.meshgrid(
                torch.linspace(0.5, height - 0.5, height, dtype=valid_ratios.dtype, device=device),
                torch.linspace(0.5, width - 0.5, width, dtype=valid_ratios.dtype, device=device),
                indexing="ij",
            )
            ref_y = ref_y.reshape(-1)[None] / (valid_ratios[:, None, lvl, 1] * height)
            ref_x = ref_x.reshape(-1)[None] / (valid_ratios[:, None, lvl, 0] * width)
            ref = torch.stack((ref_x, ref_y), -1)
            reference_points_list.append(ref)
        reference_points = torch.cat(reference_points_list, 1)
        reference_points = reference_points[:, :, None] * valid_ratios[:, None]

        return reference_points

    def forward(
        self,
        inputs_embeds=None,
        attention_mask=None,
        position_embeddings=None,
        spatial_shapes=None,
        level_start_index=None,
        valid_ratios=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
    ):
        """
        Forward pass of the Mask2FormerPixelDecoderEncoderOnly module.

        Parameters:
            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):
                Flattened feature map (output of the backbone + projection layer) that is passed to the encoder.
            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
                Mask to avoid performing attention on padding pixel features. Mask values selected in `[0, 1]`:
                - 1 for pixel features that are real (i.e., **not masked**),
                - 0 for pixel features that are padding (i.e., **masked**).
            position_embeddings (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
                Position embeddings that are added to the queries and keys in each self-attention layer.
            spatial_shapes (`torch.LongTensor` of shape `(num_levels, 2)`):
                Spatial shapes of each feature map.
            level_start_index (`torch.LongTensor` of shape `(num_levels)`, *optional*):
                Starting index of each feature map.
            valid_ratios (`torch.FloatTensor` of shape `(batch_size, num_levels, 2)`):
                Ratio of valid area in each feature level.
            output_attentions (`bool`, *optional*):
                Whether or not to return the attentions tensors of all attention layers.
            output_hidden_states (`bool`, *optional*):
                Whether or not to return the hidden states of all layers.
            return_dict (`bool`, *optional*):
                Whether or not to return a [`BaseModelOutput`] instead of a plain tuple.

        Returns:
            BaseModelOutput with the following fields:
                - last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):
                    Sequence of hidden-states at the output of the last layer of the model.
                - hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):
                    Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer).
                - attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):
                    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.
        """
        
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
        
        hidden_states = inputs_embeds
        reference_points = self.get_reference_points(spatial_shapes, valid_ratios, device=inputs_embeds.device)
        
        all_hidden_states = () if output_hidden_states else None
        all_attentions = () if output_attentions else None

        for i, encoder_layer in enumerate(self.layers):
            if output_hidden_states:
                all_hidden_states += (hidden_states.transpose(1, 0),)
                
            layer_outputs = encoder_layer(
                hidden_states,
                attention_mask,
                position_embeddings=position_embeddings,
                reference_points=reference_points,
                spatial_shapes=spatial_shapes,
                level_start_index=level_start_index,
                output_attentions=output_attentions,
            )

            hidden_states = layer_outputs[0]

            if output_attentions:
                all_attentions = all_attentions + (layer_outputs[1],)

        if output_hidden_states:
            all_hidden_states += (hidden_states.transpose(1, 0),)

        return BaseModelOutput(
            last_hidden_state=hidden_states, hidden_states=all_hidden_states, attentions=all_attentions
        )


# Modified from from transformers.models.detr.modeling_deformable_detr.DeformableDetrModel with DeformableDetrModel->Mask2FormerPixelDecoder
class Mask2FormerPixelDecoder(nn.Module):
    """
    Pixel Decoder module for Mask2Former.

    Args:
        config (Mask2FormerConfig): Configuration object containing model parameters.
        feature_channels (List[int]): List of integers representing the number of channels in each input feature map.
    """
    
    def __init__(self, config: Mask2FormerConfig, feature_channels):
        super().__init__()

        self.config = config
        feature_dim = config.feature_size

        #self.position_embedding = Mask2FormerSinePositionEmbedding(dim=feature_dim)
        self.position_embedding = LearnablePositionEmbedding2d(hei=256, wid=256, dim=feature_dim)
        self.position_projector = conv_bn_relu(feature_dim, feature_dim, kernel_size=1, is_bn=False, is_relu=False)
        
        self.num_levels = 3
        self.transformer_in_channels = feature_channels[-self.num_levels :]
        self.transformer_feature_strides = config.feature_strides[-self.num_levels :]
        self.feature_channels = feature_channels
        self.level_embed = nn.Parameter(Tensor(self.num_levels, feature_dim))

        # Create input projection layers
        self.input_projections = nn.ModuleList()
        for in_channels in self.transformer_in_channels[::-1]:
            self.input_projections.append(
                nn.Sequential(
                    nn.Conv2d(in_channels, feature_dim, kernel_size=1),
                    nn.GroupNorm(32, feature_dim),
                    )
                )

        self.encoder = Mask2FormerPixelDecoderEncoderOnly(config)

        # Extra FPN levels
        stride = min(self.transformer_feature_strides)  # 8
        self.common_stride = config.common_stride       # 4
        self.num_fpn_levels = int(np.log2(stride) - np.log2(self.common_stride))    # 1

        lateral_convs = nn.ModuleList()
        output_convs = nn.ModuleList()
        for idx, in_channels in enumerate(self.feature_channels[: self.num_fpn_levels]):
            lateral_conv = nn.Sequential(
                nn.Conv2d(in_channels, feature_dim, kernel_size=1, bias=False),
                nn.GroupNorm(32, feature_dim),
                )
            output_conv = nn.Sequential(
                nn.Conv2d(feature_dim, feature_dim, kernel_size=3, stride=1, padding=1, bias=False),
                nn.GroupNorm(32, feature_dim),
                )
            # self.add_module("adapter_{}".format(idx + 1), lateral_conv)
            # self.add_module("layer_{}".format(idx + 1), output_conv)
            lateral_convs.append(lateral_conv)
            output_convs.append(output_conv)

        # Order convolutional layers from low to high resolution
        self.lateral_convolutions = lateral_convs[::-1]
        self.output_convolutions = output_convs[::-1]

    def get_valid_ratio(self, mask, dtype=torch.float32):
        """
        Get the valid ratio of all feature maps.

        Parameters:
            mask (`torch.Tensor`): 
                Binary mask indicating valid regions of the feature maps.
            dtype (`torch.dtype`, *optional*):
                Desired data type of the returned tensor. Default: `torch.float32`.

        Returns:
            `torch.FloatTensor`: A tensor containing the valid ratios for width and height.
        """
        
        _, height, width = mask.shape
        
        valid_height = torch.sum(~mask[:, :, 0], 1)
        valid_width = torch.sum(~mask[:, 0, :], 1)
        valid_ratio_heigth = valid_height.to(dtype) / height
        valid_ratio_width = valid_width.to(dtype) / width
        valid_ratio = torch.stack([valid_ratio_width, valid_ratio_heigth], -1)
        
        return valid_ratio

    def forward(
        self,
        features,
        img_bboxes=None,
        encoder_outputs=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
    ):
        """
        Forward pass of the Mask2FormerPixelDecoder module.

        Parameters:
            features (`List[torch.Tensor]`):
                List of tensors representing the input feature maps from different levels.
            img_bboxes (`torch.Tensor`, *optional*):
                Bounding boxes for the input images, used for positional encoding.
            encoder_outputs (`torch.Tensor`, *optional*):
                Precomputed outputs from the encoder to be used instead of running the encoder again.
            output_attentions (`bool`, *optional*):
                Whether or not to return the attentions tensors of all attention layers.
            output_hidden_states (`bool`, *optional*):
                Whether or not to return the hidden states of all layers.
            return_dict (`bool`, *optional*):
                Whether or not to return a [`Mask2FormerPixelDecoderOutput`] instead of a plain tuple.

        Returns:
            Mask2FormerPixelDecoderOutput with the following fields:
                - multi_scale_features (`List[torch.FloatTensor]`):
                    Multi-scale feature maps after processing by the decoder.
                - attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):
                    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.
        """
        
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )

        # Apply 1x1 convolution to reduce the channel dimension to d_model (256 by default)
        input_embeds = []
        position_embeddings = []
        posembs = self.position_embedding(features[0], img_bboxes*0.25 if img_bboxes is not None else None, normalize=False)
        posembs = self.position_projector(posembs)
        for level, x in enumerate(features[::-1][: self.num_levels]):
            input_embeds.append(self.input_projections[level](x))
            position_embeddings.append(F.interpolate(posembs, size=x.shape[-2:], mode='bilinear', align_corners=False))
            
        masks = [torch.zeros((x.size(0), x.size(2), x.size(3)), device=x.device, dtype=torch.bool) for x in input_embeds]
        
        # Prepare encoder inputs (by flattening)
        spatial_shapes = [(embed.shape[2], embed.shape[3]) for embed in input_embeds]
        input_embeds_flat = torch.cat([embed.flatten(2).transpose(1, 2) for embed in input_embeds], 1)
        spatial_shapes = torch.as_tensor(spatial_shapes, dtype=torch.long, device=input_embeds_flat.device)
        masks_flat = torch.cat([mask.flatten(1) for mask in masks], 1)
        
        position_embeddings = [embed.flatten(2).transpose(1, 2) for embed in position_embeddings]
        level_pos_embed_flat = [x + self.level_embed[i].view(1, 1, -1) for i, x in enumerate(position_embeddings)]
        level_pos_embed_flat = torch.cat(level_pos_embed_flat, 1)

        level_start_index = torch.cat((spatial_shapes.new_zeros((1,)), spatial_shapes.prod(1).cumsum(0)[:-1]))      # cumsum:累加和
        valid_ratios = torch.stack([self.get_valid_ratio(mask, dtype=input_embeds_flat.dtype) for mask in masks], 1)

        # Send input_embeds_flat + masks_flat + level_pos_embed_flat (backbone + proj layer output) through encoder
        if encoder_outputs is None:
            encoder_outputs = self.encoder(
                inputs_embeds=input_embeds_flat,
                attention_mask=masks_flat,
                position_embeddings=level_pos_embed_flat,
                spatial_shapes=spatial_shapes,
                level_start_index=level_start_index,
                valid_ratios=valid_ratios,
                output_attentions=output_attentions,
                output_hidden_states=output_hidden_states,
                return_dict=return_dict,
            )

        last_hidden_state = encoder_outputs.last_hidden_state
        batch_size = last_hidden_state.shape[0]

        split_sizes = [None] * self.num_levels
        for i in range(self.num_levels):
            if i < self.num_levels - 1:
                split_sizes[i] = level_start_index[i + 1] - level_start_index[i]
            else:
                split_sizes[i] = last_hidden_state.shape[1] - level_start_index[i]

        encoder_output = torch.split(last_hidden_state, [size.item() for size in split_sizes], dim=1)

        # Compute final features
        outputs = [
            x.transpose(1, 2).view(batch_size, -1, spatial_shapes[i][0], spatial_shapes[i][1])
            for i, x in enumerate(encoder_output)
        ]
        
        # Append extra FPN levels to outputs, ordered from low to high resolution
        for idx, feature in enumerate(features[: self.num_fpn_levels][::-1]):
            lateral_conv = self.lateral_convolutions[idx]
            output_conv  = self.output_convolutions[idx]
            current_fpn  = lateral_conv(feature)

            # Following FPN implementation, we use nearest upsampling here
            out = current_fpn + nn.functional.interpolate(
                outputs[-1], size=current_fpn.shape[-2:], mode="bilinear", align_corners=False
            )
            out = output_conv(out)
            outputs.append(out)

        num_cur_levels = 0
        multi_scale_features = outputs
        
        output = Mask2FormerPixelDecoderOutput(
            multi_scale_features=list(multi_scale_features),
            attentions=encoder_outputs.attentions,
        )

        return output


# Linear Norm ReLU
def linear_bn_relu(in_dims, out_dims, bias=False, is_bn=True, is_relu=True):
    model = nn.Sequential()
    model.append(nn.Linear(in_dims, out_dims, bias=bias))
    if is_bn:
        model.append(nn.LayerNorm(out_dims))
    if is_relu:
        model.append(nn.SiLU())
        
    return model


# Conv2d Norm ReLU
def conv_bn_relu(in_planes, out_planes, kernel_size=3, stride=1, dilation=1, groups=1, bias=False, is_bn=True, is_relu=True):
    model = nn.Sequential()
    model.append(nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=kernel_size//2*dilation, dilation=dilation, groups=groups, bias=bias))
    if is_bn:
        model.append(nn.GroupNorm(32, out_planes))
    if is_relu:
        model.append(nn.SiLU())
        
    return model


# FusionModule_CNN
class FusionModule_CNN(nn.Module):
    """
    A CNN-based fusion module designed to merge multiple input feature maps into a single output.

    Args:
        in_planes (int, optional): 
            Number of input channels for each branch. Default is 64.
        inner_planes (int, optional): 
            Number of output channels for each convolutional layer within the branches. Default is 64.
        out_planes (int, optional): 
            Number of output channels for the final fused feature map. Default is 64.
        branch_num (int, optional): 
            Number of parallel branches used for processing inputs before fusion. Default is 2.
        dropout (float or None, optional): 
            Dropout probability value for regularization. If set to 0 or None, dropout is not applied. Default is 0.
    """
    
    def __init__(self, in_planes=64, inner_planes=64, out_planes=64, branch_num=2, dropout=0):
        super(FusionModule_CNN, self).__init__()

        self.branches = nn.ModuleList([
            conv_bn_relu(in_planes, inner_planes, kernel_size=3, is_bn=True, is_relu=False) for _ in range(branch_num)
        ])
        self.fusion = conv_bn_relu(inner_planes*branch_num, out_planes, kernel_size=3, is_bn=True, is_relu=False)
        
    def forward(self, feats, dsize=(512, 512)):
        """
        Forward pass of the FusionModule_CNN.

        Parameters:
            feats (List[torch.Tensor]): 
                List of input feature maps from different sources or scales to be fused.
            dsize (Tuple[int, int], optional): 
                Desired spatial dimensions (height, width) for the output tensor after interpolation. Default is (512, 512).

        Returns:
            torch.Tensor: The fused feature map after processing and merging all input features.
        """
        
        feats = [layer(x) for x, layer in zip(feats, self.branches)]
        feats = [nn.functional.interpolate(x, size=dsize, mode='bilinear', align_corners=False) for x in feats]
        feats = self.fusion(torch.cat(feats, dim=1))

        return feats


# Mask2FormerPixelLevelModule
class Mask2FormerPixelLevelModule(nn.Module):
    def __init__(self, config: Mask2FormerConfig):
        """
        Pixel Level Module proposed in [Masked-attention Mask Transformer for Universal Image Segmentation]
        (https://arxiv.org/abs/2112.01527). It runs the input image through a backbone and a pixel decoder, 
        generating multi-scale feature maps and pixel embeddings.

        Args:
            config ([`Mask2FormerConfig`]):
                The configuration object used to instantiate this model. It contains various parameters necessary for 
                setting up both the encoder and decoder parts of the module.
        """
        
        super().__init__()
        # Load the backbone (encoder) based on the provided configuration
        self.encoder = load_backbone(config)
        planes = self.encoder.channels

        # self.backbone = resnet101(pretrained=True, in_planes=3, replace_stride_with_dilation=[False, False, False])
        # planes = [256, 512, 1024, 2048]
        
        # Initialize the pixel decoder with the configuration and feature channels from the backbone
        self.decoder = Mask2FormerPixelDecoder(config, feature_channels=planes)
        
        # Fusion module to merge multi-scale features into a single feature map
        self.feat_merge = FusionModule_CNN(config.feature_size, config.mask_feature_size//2, config.mask_feature_size, 4)


    def forward(self, pixel_values: Tensor, img_bboxes=None, output_hidden_states: bool = False) -> Mask2FormerPixelLevelModuleOutput:
        """
        Forward pass of the Mask2FormerPixelLevelModule.

        Parameters:
            pixel_values (`torch.Tensor`): 
                A tensor representing the input images. Expected shape is `(batch_size, num_channels, height, width)`.
            img_bboxes (`torch.Tensor`, *optional*): 
                Bounding boxes for the input images, which can be used for positional encoding or other purposes. 
                Default is `None`.
            output_hidden_states (`bool`, *optional*): 
                Whether to return the hidden states from the model's layers. Default is `False`.

        Returns:
            Mask2FormerPixelLevelModuleOutput: 
                An output object containing the following fields:
                - `encoder_hidden_states`: Hidden states from the backbone (encoder).
                - `decoder_hidden_states`: Multi-scale feature maps generated by the pixel decoder.
                - `decoder_mask_features`: Final fused feature map after merging multi-scale features.
        """
        
        # Extract feature maps from the backbone (encoder)
        backbone_features = self.encoder(pixel_values).feature_maps
        #backbone_features = self.backbone(pixel_values)[1]
        #print("backbone_features shape:", [item.size() for item in backbone_features]) 
        
        # Extract feature maps from the backbone (encoder)
        decoder_output = self.decoder(backbone_features, img_bboxes=img_bboxes, output_hidden_states=output_hidden_states)
        
        # Merge the multi-scale features into a single feature map using the fusion module
        feat_fpn = self.feat_merge(decoder_output.multi_scale_features, dsize=backbone_features[0].shape[-2:])
        # print("multi_scale_features shape:", [item.size() for item in decoder_output.multi_scale_features])
        
        # Prepare the final output
        output = Mask2FormerPixelLevelModuleOutput(
            encoder_hidden_states=tuple(backbone_features),
            decoder_hidden_states=decoder_output.multi_scale_features,
            decoder_mask_features=feat_fpn,
        )
        
        return output


# A modified version of the PyTorch TransformerEncoderLayer.
class TransformerEncoderLayer(nn.TransformerEncoderLayer):
    """
    A modified version of the PyTorch TransformerEncoderLayer that supports positional embeddings.
    """
    
    def __init__(self, d_model: int, nhead: int, dim_feedforward: int = 2048, dropout: float = 0.1,
                 activation: Union[str, Callable[[Tensor], Tensor]] = F.relu,
                 layer_norm_eps: float = 1e-5, batch_first: bool = False, norm_first: bool = False,
                 device=None, dtype=None) -> None:
        super(TransformerEncoderLayer, self).__init__(d_model, nhead, dim_feedforward, dropout,
                 activation, layer_norm_eps, batch_first, norm_first, device, dtype)
        
    def with_pos_embed(self, tensor, pos: Optional[Tensor]):
        return tensor if pos is None else tensor + pos
    
    # self-attention block
    def _sa_block(self, 
                  x: Tensor, 
                  x_pos: Optional[Tensor], 
                  attn_mask: Optional[Tensor], 
                  key_padding_mask: Optional[Tensor], 
                  is_causal: bool = False) -> Tensor:
        """
        Applies self-attention mechanism on the input tensor.

        Args:
            x (Tensor): 
                Input tensor for self-attention.
            x_pos (Optional[Tensor]): 
                Positional embedding tensor for input tensor. If None, no positional embedding is added.
            attn_mask (Optional[Tensor]): 
                Mask to avoid performing attention on padding token indices. Default is None.
            key_padding_mask (Optional[Tensor]): 
                Mask to indicate which elements within the input should be ignored for the purpose of attention. 
                Default is None.
            is_causal (bool, optional): 
                If set to True, performs causal (masked) self-attention. Default is False.

        Returns:
            Tensor: 
                Output tensor after applying self-attention.
        """
        
        x = self.self_attn(query=self.with_pos_embed(x, x_pos), 
                           key=self.with_pos_embed(x, x_pos), 
                           value=x,
                           attn_mask=attn_mask,
                           key_padding_mask=key_padding_mask,
                           need_weights=False, is_causal=is_causal)[0]
        return self.dropout1(x)
        
    def forward(
        self,
        src: Tensor,
        src_pos: Optional[Tensor] = None,
        src_mask: Optional[Tensor] = None,
        src_key_padding_mask: Optional[Tensor] = None,
        is_causal: bool = False) -> Tensor:
        """
        Forward pass of the TransformerEncoderLayer.

        Args:
            src (Tensor): 
                The sequence to the encoder layer (required).
            src_pos (Optional[Tensor], optional): 
                Positional embedding tensor for src. Default is None.
            src_mask (Optional[Tensor], optional): 
                The additive mask for the src sequence. Default is None.
            src_key_padding_mask (Optional[Tensor], optional): 
                The mask for the src keys per batch. Default is None.
            is_causal (bool, optional): 
                If set to True, performs causal (masked) self-attention. Default is False.

        Returns:
            Tensor: 
                Output tensor after passing through the encoder layer.
        """
        
        x = src
        if self.norm_first:
            x = x + self._sa_block(self.norm1(x), src_pos, src_mask, src_key_padding_mask, is_causal=is_causal)
            x = x + self._ff_block(self.norm2(x))
        else:
            x = self.norm1(x + self._sa_block(x, src_pos, src_mask, src_key_padding_mask, is_causal=is_causal))
            x = self.norm2(x + self._ff_block(x))

        return x


# A modified version of the PyTorch TransformerDecoderLayer.
class TransformerDecoderLayer(nn.TransformerDecoderLayer):
    """
    A modified version of the PyTorch TransformerDecoderLayer that includes an additional self-attention block,
    a cross-attention gating mechanism, and support for positional embeddings.
    """
    
    def __init__(self, d_model: int, nhead: int, dim_feedforward: int = 2048, dropout: float = 0.1,
                 activation: Union[str, Callable[[Tensor], Tensor]] = nn.functional.silu,
                 layer_norm_eps: float = 1e-5, batch_first: bool = False, norm_first: bool = False,
                 bias: bool = True, device=None, dtype=None) -> None:
        super(TransformerDecoderLayer, self).__init__(d_model, nhead, dim_feedforward, dropout,
                 activation, layer_norm_eps, batch_first, norm_first, bias, device, dtype)
        
        # Additional self-attention block
        self.self_attn2 = nn.MultiheadAttention(
            d_model,
            nhead,
            dropout=dropout,
            batch_first=batch_first,
            bias=bias,
        )
        
        # Cross-attention gating mechanism
        self.cross_atten_gate = nn.Sequential(nn.Linear(d_model, d_model, bias=False), nn.Sigmoid(),)

    def with_pos_embed(self, tensor, pos: Optional[Tensor]):
        return tensor if pos is None else tensor + pos
    
    # self-attention block
    def _sa_block1(self, x: Tensor, 
                   x_pos: Optional[Tensor], 
                   attn_mask: Optional[Tensor], 
                   key_padding_mask: Optional[Tensor], 
                   is_causal: bool = False) -> Tensor:
        """
        Applies first self-attention mechanism on the input tensor.

        Args:
            x (Tensor): 
                Input tensor for self-attention.
            x_pos (Optional[Tensor]): 
                Positional embedding tensor for input tensor. If None, no positional embedding is added.
            attn_mask (Optional[Tensor]): 
                Mask to avoid performing attention on padding token indices. Default is None.
            key_padding_mask (Optional[Tensor]): 
                Mask to indicate which elements within the input should be ignored for the purpose of attention. 
                Default is None.
            is_causal (bool, optional): 
                If set to True, performs causal (masked) self-attention. Default is False.

        Returns:
            Tensor: 
                Output tensor after applying self-attention.
        """
        
        x = self.self_attn(query=self.with_pos_embed(x, x_pos), 
                           key=self.with_pos_embed(x, x_pos), 
                           value=x,
                           attn_mask=attn_mask,
                           key_padding_mask=key_padding_mask,
                           is_causal=is_causal,
                           need_weights=False)[0]
        return self.dropout1(x)
    
    # self-attention block
    def _sa_block2(self, x: Tensor, 
                   x_pos: Optional[Tensor], 
                   attn_mask: Optional[Tensor], 
                   key_padding_mask: Optional[Tensor], 
                   is_causal: bool = False) -> Tensor:
        """
        Applies second self-attention mechanism on the input tensor.

        Args:
            x (Tensor): 
                Input tensor for self-attention.
            x_pos (Optional[Tensor]): 
                Positional embedding tensor for input tensor. If None, no positional embedding is added.
            attn_mask (Optional[Tensor]): 
                Mask to avoid performing attention on padding token indices. Default is None.
            key_padding_mask (Optional[Tensor]): 
                Mask to indicate which elements within the input should be ignored for the purpose of attention. 
                Default is None.
            is_causal (bool, optional): 
                If set to True, performs causal (masked) self-attention. Default is False.

        Returns:
            Tensor: 
                Output tensor after applying self-attention.
        """
        
        x = self.self_attn2(query=self.with_pos_embed(x, x_pos), 
                           key=self.with_pos_embed(x, x_pos), 
                           value=x,
                           attn_mask=attn_mask,
                           key_padding_mask=key_padding_mask,
                           is_causal=is_causal,
                           need_weights=False)[0]
        return self.dropout1(x)

    # Multihead attention block
    def _mha_block(self, x: Tensor, 
                   x_pos: Optional[Tensor], 
                   mem: Tensor, 
                   mem_pos: Optional[Tensor],
                   attn_mask: Optional[Tensor], 
                   key_padding_mask: Optional[Tensor], 
                   is_causal: bool = False) -> Tensor:
        """
        Applies cross-attention between the input tensor and memory tensor.

        Args:
            x (Tensor): 
                Query tensor for cross-attention.
            x_pos (Optional[Tensor]): 
                Positional embedding tensor for query tensor. If None, no positional embedding is added.
            mem (Tensor): 
                Memory tensor for cross-attention.
            mem_pos (Optional[Tensor]): 
                Positional embedding tensor for memory tensor. If None, no positional embedding is added.
            attn_mask (Optional[Tensor]): 
                Mask to avoid performing attention on padding token indices. Default is None.
            key_padding_mask (Optional[Tensor]): 
                Mask to indicate which elements within the memory should be ignored for the purpose of attention. 
                Default is None.
            is_causal (bool, optional): 
                If set to True, performs causal (masked) cross-attention. Default is False.

        Returns:
            Tensor: 
                Output tensor after applying cross-attention.
        """
        
        x = self.multihead_attn(query=self.with_pos_embed(x, x_pos),
                                key=self.with_pos_embed(mem, mem_pos),
                                value=mem,
                                attn_mask=attn_mask,
                                key_padding_mask=key_padding_mask,
                                is_causal=is_causal,
                                need_weights=False)[0]
        return self.dropout2(x)
    
        
    def forward(
        self,
        tgt: Tensor,
        memory: Tensor,
        tgt_pos: Optional[Tensor] = None,
        memory_pos: Optional[Tensor] = None,
        tgt_mask: Optional[Tensor] = None,
        memory_mask: Optional[Tensor] = None,
        tgt_key_padding_mask: Optional[Tensor] = None,
        memory_key_padding_mask: Optional[Tensor] = None,
        tgt_is_causal: bool = False,
        memory_is_causal: bool = False,
    ) -> Tensor:
        """
        Forward pass of the TransformerDecoderLayer.

        Args:
            tgt (Tensor): 
                The sequence to the decoder layer (required).
            memory (Tensor): 
                The sequence from the encoder layer (required).
            tgt_pos (Optional[Tensor], optional): 
                Positional embedding tensor for tgt. Default is None.
            memory_pos (Optional[Tensor], optional): 
                Positional embedding tensor for memory. Default is None.
            tgt_mask (Optional[Tensor], optional): 
                The additive mask for the tgt sequence. Default is None.
            memory_mask (Optional[Tensor], optional): 
                The additive mask for the memory sequence. Default is None.
            tgt_key_padding_mask (Optional[Tensor], optional): 
                The mask for the tgt keys per batch. Default is None.
            memory_key_padding_mask (Optional[Tensor], optional): 
                The mask for the memory keys per batch. Default is None.
            tgt_is_causal (bool, optional): 
                If set to True, performs causal (masked) self-attention for tgt. Default is False.
            memory_is_causal (bool, optional): 
                If set to True, performs causal (masked) cross-attention for memory. Default is False.

        Returns:
            Tensor: 
                Output tensor after passing through the decoder layer.
        """
        
        x = tgt
        if self.norm_first:
            x = x + self._sa_block1(self.norm1(x), tgt_pos, tgt_mask, tgt_key_padding_mask, tgt_is_causal)
            memory = self._mha_block(self.norm2(x), tgt_pos, memory, memory_pos, memory_mask, memory_key_padding_mask, memory_is_causal)
            x = x + self.cross_atten_gate(x + memory) * memory
            x = x + self._sa_block2(self.norm1(x), tgt_pos, tgt_mask, tgt_key_padding_mask, tgt_is_causal)
            x = x + self._ff_block(self.norm3(x))
        else:
            x = self.norm1(x + self._sa_block1(x, tgt_pos, tgt_mask, tgt_key_padding_mask, tgt_is_causal))
            memory = self._mha_block(x, tgt_pos, memory, memory_pos, memory_mask, memory_key_padding_mask, memory_is_causal)
            x = self.norm2(x + self.cross_atten_gate(x + memory) * memory)
            x = self.norm1(x + self._sa_block2(x, tgt_pos, tgt_mask, tgt_key_padding_mask, tgt_is_causal))
            x = self.norm3(x + self._ff_block(x))

        return x


# Mask2FormerMaskedAttentionDecoderLayer_Parallel
class Mask2FormerMaskedAttentionDecoderLayer_Parallel(nn.Module):
    """
    A parallel transformer decoder layer for the Mask2Former model, designed to process instance and semantic decoding simultaneously.

    Args:
        config (`Mask2FormerConfig`):
            Configuration object containing model parameters.
        is_multi_scale (bool, optional): 
            Flag indicating whether to use multi-scale features. Default is False.
    """
    
    def __init__(self, config: Mask2FormerConfig, is_multi_scale=False):
        super().__init__()
        
        # Model dimensions and configurations
        query_hidden_dim = config.hidden_dim
        image_hidden_dim = config.feature_size
        mask_feature_dim = config.mask_feature_size
        self.num_heads = config.num_attention_heads
        self.num_levels = config.num_levels
        self.is_multi_scale = is_multi_scale
        
        image_hidden_dim = image_hidden_dim if self.is_multi_scale else mask_feature_dim
        
        # First query fusion encoder for combining instance and semantic states
        self.query_fusion_encoder_1 = TransformerEncoderLayer(d_model=query_hidden_dim, 
                                                    nhead=self.num_heads, 
                                                    dim_feedforward=config.dim_feedforward, 
                                                    batch_first=True,
                                                    norm_first=False,)

         # Instance-level feature processing layers
        self.instance_fc1 = nn.Linear(query_hidden_dim, image_hidden_dim * self.num_levels) \
            if query_hidden_dim != image_hidden_dim * self.num_levels else nn.Identity()
        self.instance_decoder = nn.ModuleList([TransformerDecoderLayer(d_model=image_hidden_dim, 
                                                    nhead=self.num_heads, 
                                                    dim_feedforward=config.dim_feedforward, 
                                                    batch_first=True,
                                                    norm_first=False,) for _ in range(self.num_levels)])
        self.instance_fc2 = nn.Linear(image_hidden_dim * self.num_levels, query_hidden_dim) \
            if query_hidden_dim != image_hidden_dim * self.num_levels else nn.Identity()
        
         # Semantic-level feature processing layers
        self.semantic_fc1 = nn.Linear(query_hidden_dim, image_hidden_dim * self.num_levels) \
            if query_hidden_dim != image_hidden_dim * self.num_levels else nn.Identity()
        self.semantic_decoder = nn.ModuleList([TransformerDecoderLayer(d_model=image_hidden_dim, 
                                                    nhead=self.num_heads, 
                                                    dim_feedforward=config.dim_feedforward, 
                                                    batch_first=True,
                                                    norm_first=False,) for _ in range(self.num_levels)])
        self.semantic_fc2 = nn.Linear(image_hidden_dim * self.num_levels, query_hidden_dim) \
            if query_hidden_dim != image_hidden_dim * self.num_levels else nn.Identity()

        # Second query fusion encoder for final combination of instance and semantic states
        self.query_fusion_encoder_2 = TransformerEncoderLayer(d_model=query_hidden_dim, 
                                                    nhead=self.num_heads, 
                                                    dim_feedforward=config.dim_feedforward, 
                                                    batch_first=True,
                                                    norm_first=False,)

    def forward(
        self,
        pixel_embeddings: Tensor = None,
        encoder_hidden_states: List[Tensor] = None,
        pixel_padding_masks: List[Tensor] = None,
        instance_states: Tensor = None,
        semantic_states: Tensor = None,
        instance_attentions: Tensor = None,
        semantic_attentions: Tensor = None,
        instance_padding_masks: Tensor = None,
        semantic_padding_masks: Tensor = None,
    ):
        """
        Forward pass of the parallel decoder layer.

        Args:
            pixel_embeddings (Tensor, optional): 
                Pixel embeddings tensor used for non-multi-scale decoding. Default is None.
            encoder_hidden_states (List[Tensor], optional): 
                List of hidden states from the encoder, used for multi-scale decoding. Default is None.
            pixel_padding_masks (List[Tensor], optional): 
                List of padding masks corresponding to the pixel embeddings or encoder hidden states. Default is None.
            instance_states (Tensor): 
                Input tensor representing instance-level features.
            semantic_states (Tensor): 
                Input tensor representing semantic-level features.
            instance_attentions (Tensor, optional): 
                Attention masks for instance-level decoding. Default is None.
            semantic_attentions (Tensor, optional): 
                Attention masks for semantic-level decoding. Default is None.
            instance_padding_masks (Tensor): 
                Padding masks for instance-level features.
            semantic_padding_masks (Tensor): 
                Padding masks for semantic-level features.

        Returns:
            List[Tensor]: 
                A list containing two tensors: one for the processed instance states and another for the processed semantic states.
        """
        
        # query fusion
        hidden_states = torch.cat((instance_states, semantic_states), dim=1)
        padding_masks = torch.cat((instance_padding_masks, semantic_padding_masks), dim=1)
        hidden_states = self.query_fusion_encoder_1(hidden_states, src_pos=None, src_mask=None, src_key_padding_mask=padding_masks)
        instance_states = hidden_states[:, :instance_states.size(1), :]
        semantic_states = hidden_states[:, instance_states.size(1):, :]
    
        # instance decoding
        if self.is_multi_scale:
            instance_states = self.instance_fc1(instance_states)
            instance_states = torch.split(instance_states, instance_states.size(-1)//self.num_levels, dim=-1)
            
            instance_states = [self.instance_decoder[level](
                    tgt=instance_states[level],
                    memory=encoder_hidden_states[level].flatten(2).transpose(1,2),
                    tgt_pos=None,
                    memory_pos=None,
                    tgt_mask=None,
                    memory_mask=instance_attentions[level],
                    tgt_key_padding_mask=instance_padding_masks,
                    memory_key_padding_mask=pixel_padding_masks[level],) for level in range(self.num_levels)]
            
            instance_states = torch.cat(instance_states, dim=-1)
            instance_states = self.instance_fc2(instance_states)
            
        else:
            instance_states = self.instance_fc1(instance_states)
            instance_states = torch.split(instance_states, instance_states.size(-1)//self.num_levels, dim=-1)
            
            instance_states = [self.instance_decoder[level](
                    tgt=instance_states[level],
                    memory=pixel_embeddings.flatten(2).transpose(1,2),
                    tgt_pos=None,
                    memory_pos=None,
                    tgt_mask=None,
                    memory_mask=instance_attentions[-1],
                    tgt_key_padding_mask=instance_padding_masks,
                    memory_key_padding_mask=pixel_padding_masks[-1],) for level in range(self.num_levels)]
            
            instance_states = torch.cat(instance_states, dim=-1)
            instance_states = self.instance_fc2(instance_states)
            
        # semantic decoding
        if self.is_multi_scale:
            semantic_states = self.semantic_fc1(semantic_states)
            semantic_states = torch.split(semantic_states, semantic_states.size(-1)//self.num_levels, dim=-1)
            
            semantic_states = [self.semantic_decoder[level](
                    tgt=semantic_states[level],
                    memory=encoder_hidden_states[level].flatten(2).transpose(1,2),
                    tgt_pos=None,
                    memory_pos=None,
                    tgt_mask=None,
                    memory_mask=semantic_attentions[level],
                    tgt_key_padding_mask=semantic_padding_masks,
                    memory_key_padding_mask=pixel_padding_masks[level],) for level in range(self.num_levels)]
            
            semantic_states = torch.cat(semantic_states, dim=-1)
            semantic_states = self.semantic_fc2(semantic_states)
            
        else:
            semantic_states = self.semantic_fc1(semantic_states)
            semantic_states = torch.split(semantic_states, semantic_states.size(-1)//self.num_levels, dim=-1)
            
            semantic_states = [self.semantic_decoder[level](
                    tgt=semantic_states[level],
                    memory=pixel_embeddings.flatten(2).transpose(1,2),
                    tgt_pos=None,
                    memory_pos=None,
                    tgt_mask=None,
                    memory_mask=semantic_attentions[-1],
                    tgt_key_padding_mask=semantic_padding_masks,
                    memory_key_padding_mask=pixel_padding_masks[-1],) for level in range(self.num_levels)]
            
            semantic_states = torch.cat(semantic_states, dim=-1)
            semantic_states = self.semantic_fc2(semantic_states)

        # query fusion
        hidden_states = torch.cat((instance_states, semantic_states), dim=1)
        padding_masks = torch.cat((instance_padding_masks, semantic_padding_masks), dim=1)
        hidden_states = self.query_fusion_encoder_2(hidden_states, src_pos=None, src_mask=None, src_key_padding_mask=padding_masks)
        instance_states = hidden_states[:, :instance_states.size(1), :]
        semantic_states = hidden_states[:, instance_states.size(1):, :]

        return [instance_states, semantic_states]


# Mask2FormerMaskedAttentionDecoderLayer_Sequential
class Mask2FormerMaskedAttentionDecoderLayer_Sequential(nn.Module):
    """
    Transformer decoder consisting of multiple layers. Each layer updates the query embeddings through cross (masked)
    and self-attention layers. This specific implementation uses a new **masked attention** mechanism to extract 
    localized features by constraining cross-attention within the foreground region of the predicted mask for each query.
    
    Args:
        config (`Mask2FormerConfig`): Configuration used to instantiate Mask2FormerMaskedAttentionDecoder.
        is_multi_scale (bool, optional): Flag indicating whether to use multi-scale processing. Defaults to False.
    """
    
    def __init__(self, config: Mask2FormerConfig, is_multi_scale=False):
        super().__init__()
        
        # Model dimensions and configurations
        query_hidden_dim = config.hidden_dim
        image_hidden_dim = config.feature_size
        mask_feature_dim = config.mask_feature_size
        self.num_heads = config.num_attention_heads
        self.num_levels = config.num_levels
        self.is_multi_scale = is_multi_scale
        
        # Adjust image_hidden_dim based on multi-scale flag
        image_hidden_dim = image_hidden_dim if self.is_multi_scale else mask_feature_dim
        
        # First query fusion encoder for initial combination of instance and semantic states
        self.query_fusion_encoder_1 = TransformerEncoderLayer(d_model=query_hidden_dim, 
                                                    nhead=self.num_heads, 
                                                    dim_feedforward=config.dim_feedforward, 
                                                    batch_first=True,
                                                    norm_first=False,)

        # Instance-level feature processing layers
        self.instance_fc1 = nn.Linear(query_hidden_dim, image_hidden_dim) \
            if query_hidden_dim != image_hidden_dim else nn.Identity()
        self.instance_decoder = nn.ModuleList([TransformerDecoderLayer(d_model=image_hidden_dim, 
                                                    nhead=self.num_heads, 
                                                    dim_feedforward=config.dim_feedforward, 
                                                    batch_first=True,
                                                    norm_first=False,) for _ in range(self.num_levels)])
        self.instance_fc2 = nn.Linear(image_hidden_dim, query_hidden_dim) \
            if query_hidden_dim != image_hidden_dim else nn.Identity()
        
        # Semantic-level feature processing layers
        self.semantic_fc1 = nn.Linear(query_hidden_dim, image_hidden_dim) \
            if query_hidden_dim != image_hidden_dim else nn.Identity()
        self.semantic_decoder = nn.ModuleList([TransformerDecoderLayer(d_model=image_hidden_dim, 
                                                    nhead=self.num_heads, 
                                                    dim_feedforward=config.dim_feedforward, 
                                                    batch_first=True,
                                                    norm_first=False,) for _ in range(self.num_levels)])
        self.semantic_fc2 = nn.Linear(image_hidden_dim, query_hidden_dim) \
            if query_hidden_dim != image_hidden_dim else nn.Identity()

        # Second query fusion encoder for final combination of instance and semantic states
        self.query_fusion_encoder_2 = TransformerEncoderLayer(d_model=query_hidden_dim, 
                                                    nhead=self.num_heads, 
                                                    dim_feedforward=config.dim_feedforward, 
                                                    batch_first=True,
                                                    norm_first=False,)

    def forward(
        self,
        pixel_embeddings: Tensor = None,
        encoder_hidden_states: List[Tensor] = None,
        pixel_padding_masks: List[Tensor] = None,
        instance_states: Tensor = None,
        semantic_states: Tensor = None,
        instance_attentions: Tensor = None,
        semantic_attentions: Tensor = None,
        instance_padding_masks: Tensor = None,
        semantic_padding_masks: Tensor = None,
        is_parallel: bool = False, 
    ):
        """
        Forward pass of the sequential decoder layer.

        Args:
            pixel_embeddings (Tensor, optional): 
                Pixel embeddings tensor used for non-multi-scale decoding. Default is None.
            encoder_hidden_states (List[Tensor], optional): 
                List of hidden states from the encoder, used for multi-scale decoding. Default is None.
            pixel_padding_masks (List[Tensor], optional): 
                List of padding masks corresponding to the pixel embeddings or encoder hidden states. Default is None.
            instance_states (Tensor): 
                Input tensor representing instance-level features.
            semantic_states (Tensor): 
                Input tensor representing semantic-level features.
            instance_attentions (Tensor, optional): 
                Attention masks for instance-level decoding. Default is None.
            semantic_attentions (Tensor, optional): 
                Attention masks for semantic-level decoding. Default is None.
            instance_padding_masks (Tensor): 
                Padding masks for instance-level features.
            semantic_padding_masks (Tensor): 
                Padding masks for semantic-level features.
            is_parallel (bool, optional): 
                Flag indicating whether to run in parallel mode. This parameter was added by xhli. Default is False.

        Returns:
            List[Tensor]: 
                A list containing two tensors: one for the processed instance states and another for the processed semantic states.
        """
        
        # query fusion
        hidden_states = torch.cat((instance_states, semantic_states), dim=1)
        padding_masks = torch.cat((instance_padding_masks, semantic_padding_masks), dim=1)
        hidden_states = self.query_fusion_encoder_1(hidden_states, src_pos=None, src_mask=None, src_key_padding_mask=padding_masks)
        instance_states = hidden_states[:, :instance_states.size(1), :]
        semantic_states = hidden_states[:, instance_states.size(1):, :]

        # instance decoding
        if self.is_multi_scale:
            instance_states = self.instance_fc1(instance_states)
            for level in range(self.num_levels):
                instance_states = self.instance_decoder[level](
                    tgt=instance_states,
                    memory=encoder_hidden_states[level].flatten(2).transpose(1,2),
                    tgt_pos=None,
                    memory_pos=None,
                    tgt_mask=None,
                    memory_mask=instance_attentions[level],
                    tgt_key_padding_mask=instance_padding_masks,
                    memory_key_padding_mask=pixel_padding_masks[level],)
            instance_states = self.instance_fc2(instance_states)
        else:
            instance_states = self.instance_fc1(instance_states)
            for level in range(self.num_levels):
                instance_states = self.instance_decoder[level](
                    tgt=instance_states,
                    memory=pixel_embeddings.flatten(2).transpose(1,2),
                    tgt_pos=None,
                    memory_pos=None,
                    tgt_mask=None,
                    memory_mask=instance_attentions[-1],
                    tgt_key_padding_mask=instance_padding_masks,
                    memory_key_padding_mask=pixel_padding_masks[-1],)
            instance_states = self.instance_fc2(instance_states)

        # semantic decoding
        if self.is_multi_scale:
            semantic_states = self.semantic_fc1(semantic_states)
            for level in range(self.num_levels):
                semantic_states = self.semantic_decoder[level](
                    tgt=semantic_states,
                    memory=encoder_hidden_states[level].flatten(2).transpose(1,2),
                    tgt_pos=None,
                    memory_pos=None,
                    memory_mask=semantic_attentions[level],
                    tgt_key_padding_mask=semantic_padding_masks,
                    memory_key_padding_mask=pixel_padding_masks[level],)      
            semantic_states = self.semantic_fc2(semantic_states)
        else:
            semantic_states = self.semantic_fc1(semantic_states)
            semantic_states = self.semantic_decoder[level](
                    tgt=semantic_states,
                    memory=pixel_embeddings.flatten(2).transpose(1,2),
                    tgt_pos=None,
                    memory_pos=None,
                    tgt_mask=None,
                    memory_mask=semantic_attentions[-1],
                    tgt_key_padding_mask=semantic_padding_masks,
                    memory_key_padding_mask=pixel_padding_masks[-1],)
            semantic_states = self.semantic_fc2(semantic_states)

        # query fusion
        hidden_states = torch.cat((instance_states, semantic_states), dim=1)
        padding_masks = torch.cat((instance_padding_masks, semantic_padding_masks), dim=1)
        hidden_states = self.query_fusion_encoder_2(hidden_states, src_pos=None, src_mask=None, src_key_padding_mask=padding_masks)
        instance_states = hidden_states[:, :instance_states.size(1), :]
        semantic_states = hidden_states[:, instance_states.size(1):, :]

        return [instance_states, semantic_states]


# Mask2FormerMaskedAttentionDecoder
class Mask2FormerMaskedAttentionDecoder(nn.Module):
    """
    Transformer decoder consisting of *config.decoder_layers* layers. Each layer is a
    [`Mask2FormerMaskedAttentionDecoderLayer`]. The decoder updates the query embeddings through multiple cross
    (masked) and self-attention layers. The decoder uses a new **masked attention** mechanism instead of the standard
    cross-attention, which extracts localized features by constraining cross-attention to within the foreground region
    of the predicted mask for each query, instead of attending to the full feature map.

    Args:
        config (`Mask2FormerConfig`):
            Configuration object containing parameters used to instantiate Mask2FormerMaskedAttentionDecoder.
    """

    def __init__(self, config: Mask2FormerConfig):
        super().__init__()

        query_hidden_dim = config.hidden_dim
        image_hidden_dim = config.feature_size
        mask_feature_dim = config.mask_feature_size
        
        self.num_heads = config.num_attention_heads
        self.num_levels = config.num_levels
        self.num_stages = config.decoder_layers
        self.query_selection = config.query_selection
        self.min_query_num = config.min_num_queries
        self.max_query_num = config.max_num_queries
        self.is_multi_scale = config.is_multi_scale
        
        image_hidden_dim = image_hidden_dim if self.is_multi_scale else mask_feature_dim
        
        # 2d position embedding
        # self.position_embed_2dim = Mask2FormerSinePositionEmbedding(dim=query_hidden_dim)
        self.position_embed_2dim = LearnablePositionEmbedding2d(hei=256, wid=256, dim=mask_feature_dim)
        self.image_emb_projector = conv_bn_relu(mask_feature_dim, image_hidden_dim, kernel_size=1, is_bn=False, is_relu=False) 
        self.mask_emb_projector  = conv_bn_relu(mask_feature_dim, mask_feature_dim, kernel_size=1, is_bn=False, is_relu=False) 
        
        # vision feature pooling
        if self.is_multi_scale:
            self.bbox_roi_size = (7, 7)
            self.bbox_feat_projection = nn.ModuleList([nn.Sequential(
                linear_bn_relu(image_hidden_dim*self.bbox_roi_size[0]*self.bbox_roi_size[1], query_hidden_dim, is_bn=True, is_relu=True),
                linear_bn_relu(query_hidden_dim, query_hidden_dim, is_bn=True, is_relu=True),
                linear_bn_relu(query_hidden_dim, query_hidden_dim, is_bn=False, is_relu=False)) for _ in range(self.num_levels)])

            self.multiscale_feat_fusion = nn.Sequential(
                linear_bn_relu(query_hidden_dim*self.num_levels, query_hidden_dim, is_bn=True, is_relu=True),
                linear_bn_relu(query_hidden_dim, query_hidden_dim, is_bn=True, is_relu=True),
                linear_bn_relu(query_hidden_dim, query_hidden_dim, is_bn=False, is_relu=False))

        else:
            self.bbox_roi_size = (7, 7)
            self.bbox_feat_projection = nn.Sequential(
                linear_bn_relu(mask_feature_dim*self.bbox_roi_size[0]*self.bbox_roi_size[1], query_hidden_dim, is_bn=True, is_relu=True),
                linear_bn_relu(query_hidden_dim, query_hidden_dim, is_bn=True, is_relu=True),
                linear_bn_relu(query_hidden_dim, query_hidden_dim, is_bn=False, is_relu=False))
        
        # query decoding
        # self.decoder_layers = nn.ModuleList([Mask2FormerMaskedAttentionDecoderLayer_Parallel(config, self.is_multi_scale) for stage in range(self.num_stages)])
        self.decoder_layers = nn.ModuleList([Mask2FormerMaskedAttentionDecoderLayer_Sequential(config, self.is_multi_scale) for stage in range(self.num_stages)])
        
        
        # semantic segmentation
        self.semantic_mask_predictor = Mask2FormerMaskPredictor(
            input_size_one = query_hidden_dim, 
            input_size_two = mask_feature_dim, 
            hidden_size = mask_feature_dim,)

        # instance segmentation
        self.instance_mask_predictor = Mask2FormerMaskPredictor(
            input_size_one = query_hidden_dim, 
            input_size_two = mask_feature_dim, 
            hidden_size = mask_feature_dim,)

        # instance detection
        self.instance_bbox_predictor = nn.Sequential(
            linear_bn_relu(query_hidden_dim, query_hidden_dim, is_bn=True, is_relu=True),
            linear_bn_relu(query_hidden_dim, query_hidden_dim, is_bn=True, is_relu=True),
            linear_bn_relu(query_hidden_dim, 4, is_bn=False, is_relu=False))
 
        # instance classification
        self.class_predictor = Mask2FormerClassPredictor(
            input_size_one = query_hidden_dim, 
            input_size_two = query_hidden_dim, 
            hidden_size = query_hidden_dim,)
        

    def mask_to_bbox(masks, img_shape=None, is_norm=False):
        """
        Converts binary masks to bounding boxes.

        Args:
            masks (Tensor): 
                A tensor of shape [N, Q, H, W] representing N images, each with Q queries, height H, and width W.
            img_shape (tuple, optional): 
                Shape of the image for normalization if `is_norm` is False and `img_shape` is provided. Default is None.
            is_norm (bool, optional): 
                Whether to normalize the bounding box coordinates between 0 and 1. Default is False.

        Returns:
            Tensor: A tensor of shape [N, Q, 4] containing the bounding boxes for each query in the format [x1, y1, x2, y2].
        """
        
        N, Q, H, W = masks.shape
        
        x_projection = masks.sum(dim=-2) > 0.5  # [N, Q, W]
        y_projection = masks.sum(dim=-1) > 0.5  # [N, Q, H]

        x1 = (x_projection.cumsum(dim=-1) == 1).float().argmax(dim=-1)
        x2 = W - 1 - ((x_projection.flip([-1])).cumsum(dim=-1) == 1).float().argmax(dim=-1)
        
        y1 = (y_projection.cumsum(dim=-1) == 1).float().argmax(dim=-1)
        y2 = H - 1 - ((y_projection.flip([-1])).cumsum(dim=-1) == 1).float().argmax(dim=-1)
        bboxes = torch.stack([x1, y1, x2, y2], dim=-1).float()
            
        if is_norm:
            bboxes[...,[0,2]] = bboxes[...,[0,2]] / W
            bboxes[...,[1,3]] = bboxes[...,[1,3]] / H
        elif img_shape is not None:
            bboxes[...,[0,2]] = bboxes[...,[0,2]] / W * img_shape[-1]
            bboxes[...,[1,3]] = bboxes[...,[1,3]] / H * img_shape[-2]
                
        return bboxes
    
    
    def bbox_to_mask(self, bboxes, img_shape):
        """
        Converts bounding boxes to binary masks.

        Args:
            bboxes (Tensor): 
                A tensor of shape [N, Q, 4] representing N images, each with Q bounding boxes in the format [x1, y1, x2, y2].
            img_shape (tuple): 
                Shape of the image (H, W).

        Returns:
            Tensor: A tensor of shape [N, Q, H, W] containing binary masks corresponding to the input bounding boxes.
        """
        
        N, Q, _ = bboxes.shape
        H, W = img_shape[-2], img_shape[-1]
        device = bboxes.device

        h_grid = torch.arange(H, device=device).view(1, 1, H, 1) + 0.5  # (1, 1, H, 1)
        w_grid = torch.arange(W, device=device).view(1, 1, 1, W) + 0.5  # (1, 1, 1, W)

        x_min = bboxes[..., 0].view(N, Q, 1, 1)  # (N, Q, 1, 1)
        y_min = bboxes[..., 1].view(N, Q, 1, 1)
        x_max = bboxes[..., 2].view(N, Q, 1, 1)
        y_max = bboxes[..., 3].view(N, Q, 1, 1)

        in_wid = (w_grid >= x_min) & (w_grid <= x_max)  # (N, Q, 1, W)
        in_hei = (h_grid >= y_min) & (h_grid <= y_max) # (N, Q, H, 1)
        masks = (in_wid & in_hei).float().clamp(min=0.01, max=0.99)  # (N, Q, H, W)

        return masks
    

    def xywh_to_xyxy(self, xywh):
        """
        Converts bounding boxes from xywh format to xyxy format.

        Args:
            xywh (Tensor): 
                A tensor of shape [..., 4] representing bounding boxes in the format [center_x, center_y, width, height].

        Returns:
            Tensor: A tensor of the same shape as input but with bounding boxes in the format [x_min, y_min, x_max, y_max].
        """
        
        cx, cy, w, h = xywh.unbind(-1)
        x1, y1 = cx - w / 2, cy - h / 2
        x2, y2 = cx + w / 2, cy + h / 2 
        
        return torch.stack((x1, y1, x2, y2), dim=-1)


    def xyxy_to_xywh(self, xyxy):
        """
        Converts bounding boxes from xyxy format to xywh format.

        Args:
            xyxy (Tensor): 
                A tensor of shape [..., 4] representing bounding boxes in the format [x_min, y_min, x_max, y_max].

        Returns:
            Tensor: A tensor of the same shape as input but with bounding boxes in the format [center_x, center_y, width, height].
        """
        
        x1,y1,x2,y2 = xyxy.unbind(-1)
        cx, cy = (x1 + x2) / 2, (y1 + y2) / 2
        w, h = x2 - x1, y2 - y1
        
        return torch.stack((cx, cy, w, h), dim=-1)


    def get_attention_mask(self, masks, num_heads, feature_size_list):
        """
        Generates attention masks based on the input binary masks.

        Args:
            masks (Tensor): 
                A tensor of shape [N, Q, H, W] representing binary masks.
            num_heads (int): 
                Number of attention heads.
            feature_size_list (List[tuple]): 
                List of target sizes for resizing the attention masks.

        Returns:
            List[Tensor]: A list of attention masks resized according to the `feature_size_list`.
        """
        
        if masks is None:
            attention_masks = [None for item in feature_size_list]
            return attention_masks
        
        attention_masks = []
        for target_size in feature_size_list:
            scale = round(masks.size(2) / target_size[0])
            attention_mask = nn.functional.max_pool2d(masks, kernel_size=(scale*2+1,scale*2+1), stride=(1,1), padding=(scale,scale)) 
            attention_mask = nn.functional.interpolate(attention_mask, size=target_size, mode="bilinear", align_corners=False)
            attention_mask = (attention_mask.sigmoid() < 0.5).flatten(2) # True for mask
            attention_mask[torch.where(attention_mask.sum(-1) == attention_mask.shape[-1])] = False
            attention_mask = attention_mask.unsqueeze(1).expand(attention_mask.shape[0], num_heads, masks.size(1), attention_mask.shape[2]).flatten(0, 1)
            attention_masks.append(attention_mask)
        
        return attention_masks


    def get_padding_mask(self, masks, feature_size_list):
        """
        Generates padding masks based on the input binary masks.

        Args:
            masks (Tensor): 
                A tensor of shape [N, Q, H, W] representing binary masks.
            feature_size_list (List[tuple]): 
                List of target sizes for resizing the padding masks.

        Returns:
            List[Tensor]: A list of padding masks resized according to the `feature_size_list`.
        """
        
        padding_masks = []
        for target_size in feature_size_list:
            padding_mask = nn.functional.interpolate(masks, size=target_size, mode="bilinear", align_corners=False)
            padding_mask = padding_mask < 0.5 # True for mask
            padding_mask = padding_mask.flatten(1)
            padding_masks.append(padding_mask)
        
        return padding_masks


    def multi_class_nms(self, states, bboxes, maskes, scores, labels, iou_threshold=0.5): 
        """
        Performs Non-Maximum Suppression (NMS) on bounding boxes for each class separately.

        Args:
            states (Tensor, optional): 
                A tensor of shape [Q, D] representing the states/embeddings of instances, where N is the batch size,
                Q is the number of queries, and D is the dimensionality of the states. Can be None if not applicable.
            bboxes (Tensor): 
                A tensor of shape [Q, 4] representing bounding boxes in the format [x1, y1, x2, y2].
            maskes (Tensor, optional): 
                A tensor of shape [Q, H, W] representing binary masks corresponding to the bounding boxes. Can be None if not applicable.
            scores (Tensor): 
                A tensor of shape [Q] containing confidence scores for each bounding box.
            labels (Tensor): 
                A tensor of shape [Q] containing class labels for each bounding box.
            iou_threshold (float, optional): 
                Intersection over Union (IoU) threshold used for NMS. Default is 0.5.

        Returns:
            Tuple[Tensor]: A tuple containing filtered states, bounding boxes, masks, scores, and labels after applying NMS.
        """
    
        selected_states = []
        selected_bboxes = []
        selected_maskes = []
        selected_scores = []
        selected_labels = []

        unique_labels = torch.unique(labels)  
        for label in unique_labels:  
            current_states = states[labels==label] if states is not None else None
            current_bboxes = bboxes[labels==label]
            current_maskes = maskes[labels==label] if maskes is not None else None
            current_scores = scores[labels==label]
            current_labels = labels[labels==label]
             
            keep_indices = torchvision.ops.nms(current_bboxes, current_scores, iou_threshold)  

            selected_states.append(current_states[keep_indices]) if states is not None else None
            selected_bboxes.append(current_bboxes[keep_indices])  
            selected_maskes.append(current_maskes[keep_indices]) if maskes is not None else None
            selected_scores.append(current_scores[keep_indices])   
            selected_labels.append(current_labels[keep_indices]) 
        
        selected_states = torch.cat(selected_states, dim=0) if states is not None else None
        selected_bboxes = torch.cat(selected_bboxes, dim=0)
        selected_maskes = torch.cat(selected_maskes, dim=0) if maskes is not None else None
        selected_scores = torch.cat(selected_scores, dim=0)
        selected_labels = torch.cat(selected_labels, dim=0)

        return selected_states, selected_bboxes, selected_maskes, selected_scores, selected_labels


    def select_queries(self, instance_states, instance_bboxes, instance_maskes, instance_logits, class_names, score_thres=0.5, min_num=10, max_num=900):
        """
        Selects a subset of queries based on classification scores, Non-Maximum Suppression (NMS), and top-k filtering.

        Args:
            instance_states (Tensor, optional): 
                A tensor of shape [N, Q, D] representing instance embeddings/states, where N is the batch size, Q is the number of queries,
                and D is the dimensionality of the states. Can be None if not applicable.
            instance_bboxes (Tensor): 
                A tensor of shape [N, Q, 4] representing bounding boxes in the format [x1, y1, x2, y2].
            instance_maskes (Tensor, optional): 
                A tensor of shape [N, Q, H, W] representing binary masks corresponding to the bounding boxes. Can be None if not applicable.
            instance_logits (Tensor): 
                A tensor of shape [N, Q, C] representing logits from the classifier, where C is the number of classes.
            class_names (List[List[str]]): 
                A list of lists containing class names for each image in the batch.
            score_thres (float, optional): 
                Confidence score threshold for selecting queries. Default is 0.5.
            min_num (int, optional): 
                Minimum number of queries to select per image. Default is 10.
            max_num (int, optional): 
                Maximum number of queries to select per image. Default is 900.

        Returns:
            Tuple[Tensor]: A tuple containing selected states, bounding boxes, masks, and padding masks after filtering and padding.
        """
    
        N, Q, D = instance_states.size()

        selected_states = []
        selected_bboxes = []
        selected_maskes = []
        for n in range(N):
            instance_states_n = instance_states[n,...] if instance_states is not None else None
            instance_bboxes_n = instance_bboxes[n,...]
            instance_maskes_n = instance_maskes[n,...] if instance_maskes is not None else None
            instance_logits_n = instance_logits[n,...]
            
            # query selection based on threshold
            num_labels = len(class_names[n])
            instance_scores_n = instance_logits_n[:,:num_labels].softmax(dim=-1)
            instance_scores_n, instance_labels_n = instance_scores_n[...,:num_labels-1].max(dim=-1)
            indexes = instance_scores_n > score_thres
            if indexes.sum() < min_num:
                indexes = instance_scores_n.topk(k=min(min_num, indexes.size(0)), dim=-1)[1]
            instance_states_n = instance_states_n[indexes] if instance_states is not None else None
            instance_bboxes_n = instance_bboxes_n[indexes]
            instance_maskes_n = instance_maskes_n[indexes] if instance_maskes is not None else None
            instance_scores_n = instance_scores_n[indexes]
            instance_labels_n = instance_labels_n[indexes]
            
            # query selection based on bbox nms
            instance_states_n, instance_bboxes_n, instance_maskes_n, instance_scores_n, instance_labels_n = self.multi_class_nms(\
                instance_states_n, instance_bboxes_n, instance_maskes_n, instance_scores_n, instance_labels_n, iou_threshold=0.9)

            # query selection based on topk
            if instance_states_n.size(0) > max_num:
                indexes = instance_scores_n.topk(k=max_num, dim=-1)[1]
                instance_states_n = instance_states_n[indexes] if instance_states is not None else None
                instance_bboxes_n = instance_bboxes_n[indexes]
                instance_maskes_n = instance_maskes_n[indexes] if instance_maskes is not None else None
                instance_scores_n = instance_scores_n[indexes]
                instance_labels_n = instance_labels_n[indexes]

            selected_states.append(instance_states_n) if instance_states is not None else None
            selected_bboxes.append(instance_bboxes_n)
            selected_maskes.append(instance_maskes_n) if instance_maskes is not None else None

        padding_masks = [torch.zeros(item.size(0)).to(item.device) for item in selected_states]
        padding_masks = pad_sequence(padding_masks, batch_first=True, padding_value=1).bool()
        selected_states = pad_sequence(selected_states, batch_first=True, padding_value=0) if instance_states is not None else None
        selected_bboxes = pad_sequence(selected_bboxes, batch_first=True, padding_value=0)
        selected_maskes = pad_sequence(selected_maskes, batch_first=True, padding_value=0) if instance_maskes is not None else None

        return selected_states, selected_bboxes, selected_maskes, padding_masks
    
    
    def forward(
        self,
        class_names: List[list[str]],
        instance_bboxes: Tensor = None,
        instance_states: Tensor = None,
        semantic_states: Tensor = None,
        instance_padding_masks: Tensor = None,
        semantic_padding_masks: Tensor = None,
        pixel_embeddings: Tensor = None,
        pixel_mask: Tensor = None,
        img_bboxes: Tensor = None,
        encoder_hidden_states: Tensor = None,
        output_hidden_states: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ):
        """
        Forward pass of the Mask2FormerMaskedAttentionDecoder model.

        Args:
            class_names (List[list[str]]): 
                A list of lists containing class names for each image in the batch.
            instance_bboxes (Tensor, optional): 
                A tensor of shape [N, Q, 4] representing bounding boxes for instances, where N is the batch size and Q is the number of queries. Can be None if not applicable.
            instance_states (Tensor, optional): 
                A tensor of shape [N, Q, D] representing embeddings or states of instances, where D is the dimensionality of the states. Can be None if not applicable.
            semantic_states (Tensor, optional): 
                A tensor of shape [N, C, D] representing semantic states or embeddings, where C is the max class number in a batch. Can be None if not applicable.
            instance_padding_masks (Tensor, optional): 
                A tensor indicating which elements in `instance_states` should be ignored due to padding. Can be None if not applicable.
            semantic_padding_masks (Tensor, optional): 
                A tensor indicating which elements in `semantic_states` should be ignored due to padding. Can be None if not applicable.
            pixel_embeddings (Tensor): 
                A tensor of shape [N, C, H, W] representing pixel-wise embeddings, where C is the number of channels.
            pixel_mask (Tensor, optional): 
                A tensor indicating valid pixels within `pixel_embeddings`. Can be None if not applicable.
            img_bboxes (Tensor, optional): 
                A tensor of shape [N, 4] representing bounding boxes for images. Can be None if not applicable.
            encoder_hidden_states (Tensor): 
                A list of tensors representing multi-scale image features from the pixel-level module, used as context for decoding.
            output_hidden_states (Optional[bool], optional): 
                Whether to return all intermediate hidden states. Default is None.
            output_attentions (Optional[bool], optional): 
                Whether to return attention probabilities. Default is None.
            return_dict (Optional[bool], optional): 
                Whether to return outputs as a dictionary or a tuple. Default is None.

        Returns:
            Mask2FormerMaskedAttentionDecoderOutput: 
                An object containing all intermediate states, masks, bounding box predictions, and classification predictions after the forward pass.
        """
    
        N, C, H, W = pixel_embeddings.size()

        # position embedding
        posembs = self.position_embed_2dim(pixel_embeddings, img_bboxes*0.25 if img_bboxes is not None else None, normalize=False)
        mask_posembs  = self.mask_emb_projector(posembs)
        image_posembs = self.image_emb_projector(posembs)
        image_posembs = [F.interpolate(image_posembs, size=item.shape[-2:], mode='bilinear', align_corners=False) for item in encoder_hidden_states]

        pixel_embeddings = pixel_embeddings + mask_posembs
        encoder_hidden_states = [feat + posi for feat, posi in zip(encoder_hidden_states, image_posembs)]

        # pixel padding mask
        feature_size_list = [item.shape[-2:] for item in encoder_hidden_states]
        pixel_padding_masks = self.get_padding_mask(pixel_mask, feature_size_list)

        # intermediate hidden states
        all_instance_states = []
        all_semantic_states = []
        
        # intermediate predictions
        all_instance_masks = []
        all_semantic_masks = []
        all_bbox_predictions = []
        all_cate_predictions = []
        
        instance_masks = None
        semantic_masks = None

        for idx in range(self.num_stages + 1):
            # query selection
            if idx > 0 and self.query_selection:
                score_thres = 0.01 / pow(2, self.num_stages - idx)
                instance_states, instance_bboxes, instance_masks, instance_padding_masks = self.select_queries(instance_states, instance_bboxes, \
                    instance_masks, instance_logits, class_names, score_thres, min_num=self.min_query_num, max_num=self.max_query_num)

            # feature pooling
            N, Q, C = instance_bboxes.size()
            rois = [item[0] for item in instance_bboxes.split(dim=0, split_size=1)]
            if self.is_multi_scale:
                roi_feat = [torchvision.ops.roi_align(item, rois, output_size=self.bbox_roi_size, spatial_scale=1/4/pow(2, 3-level)) \
                    for level, item in enumerate(encoder_hidden_states)]
                roi_feat = [self.bbox_feat_projection[level](item.flatten(1)).view(N, Q, -1) for level, item in enumerate(roi_feat)]
                roi_feat = self.multiscale_feat_fusion(torch.cat(roi_feat, dim=-1))
                instance_states = 0.5 * instance_states + 0.5 * roi_feat
            else:
                roi_feat = torchvision.ops.roi_align(pixel_embeddings, rois, output_size=self.bbox_roi_size, spatial_scale=1/4)
                roi_feat = self.bbox_feat_projection(roi_feat.flatten(1)).view(N, Q, -1)
                instance_states = 0.5 * instance_states + 0.5 * roi_feat

            if idx > 0:
                # attention masks
                instance_attentions = self.get_attention_mask(instance_masks, self.num_heads, feature_size_list)
                semantic_attentions = self.get_attention_mask(semantic_masks, self.num_heads, feature_size_list)
                
                # query decoding 
                instance_states, semantic_states = self.decoder_layers[idx - 1](
                        pixel_embeddings = pixel_embeddings,
                        encoder_hidden_states = encoder_hidden_states,
                        pixel_padding_masks = pixel_padding_masks,
                        instance_states = instance_states,
                        semantic_states = semantic_states,
                        instance_attentions = instance_attentions,
                        semantic_attentions = semantic_attentions,
                        instance_padding_masks = instance_padding_masks,
                        semantic_padding_masks = semantic_padding_masks,
                    )
                

            if not self.training:
                print(idx, instance_states.size())

            # semantic segmentation
            semantic_masks, semantic_feats = self.semantic_mask_predictor(semantic_states, pixel_embeddings)

            # instance segmentation
            if idx > 0:
                instance_masks, instance_feats = self.instance_mask_predictor(instance_states, pixel_embeddings)

            # instance detection
            instance_bboxes = instance_bboxes + self.instance_bbox_predictor(instance_states) * 100
      
            # instance classification
            instance_logits = self.class_predictor(instance_states, semantic_states)
            

            all_instance_states  += [instance_states,]
            all_semantic_states  += [semantic_states,]
            all_instance_masks   += [instance_masks,]
            all_semantic_masks   += [semantic_masks,]
            all_bbox_predictions += [instance_bboxes,]
            all_cate_predictions += [instance_logits,]
            

        output = Mask2FormerMaskedAttentionDecoderOutput(
            all_instance_states=all_instance_states,
            all_semantic_states=all_semantic_states,
            all_instance_masks=all_instance_masks,
            all_semantic_masks=all_semantic_masks,
            all_bbox_predictions=all_bbox_predictions,
            all_cate_predictions=all_cate_predictions,
        )

        return output
    

# Mask2FormerMaskPredictor
class Mask2FormerMaskPredictor(nn.Module):
    """
    A module for predicting masks in the Mask2Former model. It processes query features and pixel embeddings to generate mask logits 
    and pooled features which can be used for segmentation tasks.
    """
    
    def __init__(self, input_size_one: int, input_size_two: int, hidden_size: int):
        """
        Initializes the Mask2FormerMaskPredictor with the specified sizes and number of heads.

        Args:
            input_size_one (int): The size of the first input dimension, typically related to query features.
            input_size_two (int): The size of the second input dimension, related to visual/pixel embeddings.
            hidden_size (int): The size of the hidden layers within the network.
        """
        
        super().__init__()

        # Reduces and transforms query features into a lower-dimensional space.
        self.query_feat_reducer = nn.Sequential(
            linear_bn_relu(input_size_one, input_size_one, is_bn=True, is_relu=True),
            linear_bn_relu(input_size_one, input_size_one, is_bn=True, is_relu=True),
            linear_bn_relu(input_size_one, hidden_size, is_bn=False, is_relu=False))

        # Reduces and transforms pixel embeddings into a lower-dimensional space suitable for combining with queries.
        self.visual_feat_reducer = nn.Sequential(
            conv_bn_relu(input_size_two, input_size_two, kernel_size=3, is_bn=True, is_relu=True),
            conv_bn_relu(input_size_two, input_size_two, kernel_size=3, is_bn=True, is_relu=True),
            conv_bn_relu(input_size_two, hidden_size, kernel_size=3, is_bn=False, is_relu=False),)

        # Further processing of pooled features through fully connected layers.
        self.pixel_feat_reducer = nn.Sequential(
            linear_bn_relu(hidden_size, input_size_one, is_bn=True, is_relu=True),
            linear_bn_relu(input_size_one, input_size_one, is_bn=True, is_relu=True),
            linear_bn_relu(input_size_one, input_size_one, is_bn=False, is_relu=False))
        
        
    def forward(self, queries: Tensor, pixel_embeddings: Tensor):
        """
        Processes the inputs to predict masks and compute pooled features.

        Args:
            queries (Tensor): Query features tensor with shape (batch_size, num_queries, input_size_one).
            pixel_embeddings (Tensor): Pixel embeddings tensor with shape (batch_size, input_size_two, height, width).

        Returns:
            Tuple[Tensor, Tensor]: 
                - outputs_mask (Tensor): Predicted mask logits with shape (batch_size, num_queries, height, width).
                - pooled_feats (Tensor): Pooled features based on predicted masks with shape (batch_size, num_queries, input_size_one).
        """
        
        queries = self.query_feat_reducer(queries)
        pixel_embeddings = self.visual_feat_reducer(pixel_embeddings)
        outputs_mask = torch.einsum("bqc, bchw -> bqhw", queries, pixel_embeddings)

        pixel_weight = outputs_mask.sigmoid() / (outputs_mask.sigmoid().sum((2, 3))[:,:,None,None] + 1e-10)
        pooled_feats = torch.einsum("bqhw, bchw -> bqc", pixel_weight, pixel_embeddings) 
        pooled_feats = self.pixel_feat_reducer(pooled_feats)
    
        return outputs_mask, pooled_feats
    
    
# Mask2FormerClassPredictor
class Mask2FormerClassPredictor(nn.Module):
    """
    A module for predicting class logits in the Mask2Former model. It processes instance queries and semantic queries to generate 
    semantic logits, which can be used for classification tasks.
    """
    
    def __init__(self, input_size_one: int, input_size_two: int, hidden_size: int,):
        """
        Initializes the Mask2FormerClassPredictor with specified sizes.

        Args:
            input_size_one (int): The size of the first input dimension, typically related to instance queries.
            input_size_two (int): The size of the second input dimension, related to semantic queries.
            hidden_size (int): The size of the hidden layers within the network.
        """
        
        super().__init__()
        
        # Reduces and transforms instance queries into a lower-dimensional space.
        self.feat_reducer_one = nn.Sequential(
            linear_bn_relu(input_size_one, input_size_one, is_bn=True, is_relu=True),
            linear_bn_relu(input_size_one, input_size_one, is_bn=True, is_relu=True), 
            linear_bn_relu(input_size_one, hidden_size, is_bn=False, is_relu=False))
        
        # Reduces and transforms semantic queries into a lower-dimensional space suitable for combining with instance queries.
        self.feat_reducer_two = nn.Sequential(
            linear_bn_relu(input_size_two, input_size_two, is_bn=True, is_relu=True),
            linear_bn_relu(input_size_two, input_size_two, is_bn=True, is_relu=True),
            linear_bn_relu(input_size_two, hidden_size, is_bn=False, is_relu=False))


    def forward(self, instance_queries: Tensor, semantic_queries: Tensor):
        """
        Processes the inputs to predict semantic logits.

        Args:
            instance_queries (Tensor): Instance queries tensor with shape (batch_size, num_instance_queries, input_size_one).
            semantic_queries (Tensor): Semantic queries tensor with shape (batch_size, num_semantic_queries, input_size_two).

        Returns:
            Tensor: 
                - semantic_logits (Tensor): Predicted semantic logits with shape (batch_size, num_instance_queries, num_semantic_queries).
        """
        
        instance_queries = self.feat_reducer_one(instance_queries)
        semantic_queries = self.feat_reducer_two(semantic_queries)
        semantic_logits = torch.einsum('bic,bjc->bij', instance_queries, semantic_queries)

        return semantic_logits
    
    
from torch.nn.utils.rnn import pad_sequence
from transformers import AutoConfig, AutoTokenizer, AutoModel, logging

# Mask2FormerTransformerModule
class Mask2FormerTransformerModule(nn.Module):
    """
    The transformer module for Mask2Former, which processes multi-scale features and mask features to generate predictions.
    """

    def __init__(self, in_features: int, config: Mask2FormerConfig):
        """
        Initializes the Mask2FormerTransformerModule with specified input features and configuration.

        Args:
            in_features (int): Number of channels in the input features.
            config (Mask2FormerConfig): Configuration object containing parameters for the model.
        """
        
        super().__init__()
        
        # Configuration settings
        query_hidden_dim = config.hidden_dim
        image_hidden_dim = config.feature_size
        mask_feature_dim = config.mask_feature_size
        self.num_heads = config.num_attention_heads
        self.num_queries = config.num_queries
        self.num_levels = config.num_levels

        # Vision feature projection layers for each level of the pyramid
        self.input_projections = nn.ModuleList([conv_bn_relu(in_features, image_hidden_dim, kernel_size=3, is_bn=True, is_relu=False) \
            if in_features != image_hidden_dim else nn.Identity() for _ in range(self.num_levels)])

        # Text encoder for semantic queries generation
        model_path = "/local_data1/xhli/data4/tools/pretrained_model/transformers/sentence/all-MiniLM-L6-v2"
        # model_path = "/local_data1/xhli/data4/tools/pretrained_model/transformers/sentence/paraphrase-multilingual-MiniLM-L12-v2"

        self.textual_config = AutoConfig.from_pretrained(model_path)
        self.textual_tokenizer = AutoTokenizer.from_pretrained(model_path, config=self.textual_config)
        self.textual_encoder = AutoModel.from_pretrained(model_path, config=self.textual_config, ignore_mismatched_sizes=True)
        self.semantic_query_projection = nn.Linear(384, query_hidden_dim)

        # Initial instance bounding boxes and queries
        self.proposal_bboxes  = nn.Embedding(config.num_queries, 4)
        self.instance_queries = nn.Embedding(config.num_queries, query_hidden_dim)
        self.instance_posembs = nn.Embedding(config.num_queries, query_hidden_dim)

        # Masked decoder for final prediction
        self.decoder = Mask2FormerMaskedAttentionDecoder(config=config)

    
    def extract_text_feature(self, texts, device):
        """
        Extracts text features from given texts using a pre-trained text encoder.

        Args:
            texts (list[str]): List of text descriptions.
            device (torch.device): Device to move tensors to.

        Returns:
            tuple[Tensor, Tensor]: Mean-pooled embeddings and CLS token embeddings.
        """
        
        inputs = self.textual_tokenizer(texts, padding=True, return_tensors="pt").to(device)
        token_embeddings = self.textual_encoder(inputs["input_ids"], inputs["attention_mask"])[0]
        
        attention_masks = inputs["attention_mask"][...,None].expand(token_embeddings.size()).float()
        sum_embeddings = torch.sum(token_embeddings * attention_masks, dim=1)
        sum_mask = torch.clamp(attention_masks.sum(dim=1), min=1)
        embedding_avg = sum_embeddings / sum_mask    # mean pooling
        embedding_cls = token_embeddings[:,0,:]      # CLS token 
        
        return embedding_avg, embedding_cls


    def xywh_to_xyxy(self, xywh):
        cx, cy, w, h = xywh.unbind(-1)
        x1, y1 = cx - w / 2, cy - h / 2
        x2, y2 = cx + w / 2, cy + h / 2 
        
        return torch.stack((x1, y1, x2, y2), dim=-1)


    def with_pos_embed(self, tensor, pos: Optional[Tensor]):
        
        return tensor if pos is None else tensor + pos
    
    
    def pad_sequence(self, tensors):
        padded_tensors = pad_sequence(tensors, batch_first=True, padding_value=0)
        masks = [torch.zeros(item.size(0)).to(item.device) for item in tensors]
        masks = pad_sequence(masks, batch_first=True, padding_value=1).bool()
  
        return padded_tensors, masks
    

    def forward(
        self,
        multi_scale_features: List[Tensor],
        mask_features: Tensor,
        class_names: List[list[str]],
        pixel_mask: Optional[Tensor] = None,
        img_bboxes: Optional[Tensor] = None,
        output_hidden_states: bool = False,
        output_attentions: bool = False,
    ) -> Mask2FormerMaskedAttentionDecoderOutput:
        """
        Forward pass through the transformer module.

        Args:
            multi_scale_features (List[Tensor]): A list of tensors representing multi-scale visual features.
            mask_features (Tensor): Tensor representing mask features.
            class_names (List[list[str]]): A nested list of class names for generating semantic queries.
            pixel_mask (Optional[Tensor], optional): Pixel-level mask. Defaults to None.
            img_bboxes (Optional[Tensor], optional): Image bounding boxes. Defaults to None.
            output_hidden_states (bool, optional): Whether to return hidden states. Defaults to False.
            output_attentions (bool, optional): Whether to return attentions. Defaults to False.

        Returns:
            Mask2FormerMaskedAttentionDecoderOutput: An object containing the outputs of the masked attention decoder.
        """
        
        # Process vision features at multiple scales
        encoder_hidden_states = []
        for i in range(self.num_levels):
            vision_feature = self.input_projections[i](multi_scale_features[i])
            encoder_hidden_states.append(vision_feature)
            
        # Generate semantic queries based on class names
        with torch.no_grad():
            semantic_queries = [self.extract_text_feature(item, mask_features.device)[0] for item in class_names]
        semantic_queries = pad_sequence(semantic_queries, batch_first=True, padding_value=0)
        semantic_queries = self.semantic_query_projection(semantic_queries)
        semantic_states  = self.with_pos_embed(semantic_queries, None)
        semantic_padding_masks = [torch.zeros(len(item)).to(semantic_states) for item in class_names]
        semantic_padding_masks = pad_sequence(semantic_padding_masks, batch_first=True, padding_value=1).bool()

        # Generate initial instance queries and positional encodings
        N, C, H, W = mask_features.size()
        instance_ids = torch.arange(self.num_queries).to(mask_features.device)
        instance_queries = self.instance_queries(instance_ids)[None,:,:].repeat(N, 1, 1)
        instance_posembs = self.instance_posembs(instance_ids)[None,:,:].repeat(N, 1, 1)
        instance_states  = self.with_pos_embed(instance_queries, instance_posembs)
        instance_padding_masks = torch.zeros(instance_states.shape[:2]).bool().to(instance_states)

        # Generate initial instance bounding boxes
        N, C, H, W = pixel_mask.size()
        instance_ids = torch.arange(self.num_queries).to(pixel_mask.device)
        instance_bboxes = self.proposal_bboxes(instance_ids)
        instance_bboxes[:, 2] = instance_bboxes[:, 2] / 4
        instance_bboxes[:, 3] = instance_bboxes[:, 3] / 4
        instance_bboxes = self.xywh_to_xyxy(instance_bboxes)
        instance_bboxes = instance_bboxes[None,:,:].repeat(N, 1, 1)
        if img_bboxes is None:
            instance_bboxes[..., [0,2]] = instance_bboxes[..., [0,2]] * W
            instance_bboxes[..., [1,3]] = instance_bboxes[..., [1,3]] * H
        else:
            for im, img_bbox in enumerate(img_bboxes):
                instance_bboxes[im, :, [0,2]] = instance_bboxes[im, :, [0,2]] * (img_bbox[2] - img_bbox[0]) + img_bbox[0]
                instance_bboxes[im, :, [1,3]] = instance_bboxes[im, :, [1,3]] * (img_bbox[3] - img_bbox[1]) + img_bbox[1]

        # Decode and predict using the masked attention decoder
        decoder_output = self.decoder(
            class_names=class_names,
            instance_bboxes=instance_bboxes,
            instance_states=instance_states,
            semantic_states=semantic_states,
            instance_padding_masks=instance_padding_masks,
            semantic_padding_masks=semantic_padding_masks,
            pixel_embeddings=mask_features,
            pixel_mask=pixel_mask,
            img_bboxes=img_bboxes,
            encoder_hidden_states=encoder_hidden_states,
            output_hidden_states=output_hidden_states,
            output_attentions=output_attentions,
            return_dict=True,
        )

        return decoder_output


# Mask2FormerPreTrainedModel
class Mask2FormerPreTrainedModel(PreTrainedModel):
    config_class = Mask2FormerConfig
    base_model_prefix = "model"
    main_input_name = "pixel_values"

    def _init_weights(self, module: nn.Module):
        xavier_std = self.config.init_xavier_std
        std = self.config.init_std

        if isinstance(module, Mask2FormerTransformerModule):
            if module.input_projections is not None:
                for input_projection in module.input_projections:
                    if not isinstance(input_projection, nn.Identity):
                        nn.init.xavier_uniform_(input_projection.weight, gain=xavier_std)
                        nn.init.constant_(input_projection.bias, 0)

        elif isinstance(module, Mask2FormerPixelDecoderEncoderMultiscaleDeformableAttention):
            nn.init.constant_(module.sampling_offsets.weight.data, 0.0)
            thetas = torch.arange(module.n_heads, dtype=torch.int64).float() * (2.0 * math.pi / module.n_heads)
            grid_init = torch.stack([thetas.cos(), thetas.sin()], -1)
            grid_init = (
                (grid_init / grid_init.abs().max(-1, keepdim=True)[0])
                .view(module.n_heads, 1, 1, 2)
                .repeat(1, module.n_levels, module.n_points, 1)
            )
            for i in range(module.n_points):
                grid_init[:, :, i, :] *= i + 1
            with torch.no_grad():
                module.sampling_offsets.bias = nn.Parameter(grid_init.view(-1))

            nn.init.constant_(module.attention_weights.weight.data, 0.0)
            nn.init.constant_(module.attention_weights.bias.data, 0.0)
            nn.init.xavier_uniform_(module.value_proj.weight.data)
            nn.init.constant_(module.value_proj.bias.data, 0.0)
            nn.init.xavier_uniform_(module.output_proj.weight.data)
            nn.init.constant_(module.output_proj.bias.data, 0.0)

        elif isinstance(module, Mask2FormerPixelLevelModule):
            for submodule in module.modules():
                if isinstance(submodule, (nn.Conv2d, nn.Linear)):
                    submodule.weight.data.normal_(mean=0.0, std=std)
                    if submodule.bias is not None:
                        submodule.bias.data.zero_()

        elif isinstance(module, Mask2FormerPixelDecoder):
            for p in module.parameters():
                if p.dim() > 1:
                    nn.init.xavier_uniform_(p)
            nn.init.normal_(module.level_embed, std=0)

        elif isinstance(module, Mask2FormerPixelDecoderEncoderOnly):
            for p in module.parameters():
                if p.dim() > 1:
                    nn.init.xavier_uniform_(p)

        elif isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):
            module.weight.data.normal_(mean=0.0, std=std)
            if module.bias is not None:
                module.bias.data.zero_()

        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=0.0, std=std)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()
            # instance proposal initialization
            if module.weight.size(-1) == 4:
                module.weight.data.uniform_()

        if hasattr(module, "reference_points"):
            nn.init.xavier_uniform_(module.reference_points.weight.data, gain=1.0)
            nn.init.constant_(module.reference_points.bias.data, 0.0)


# Mask2FormerModel
class Mask2FormerModel(Mask2FormerPreTrainedModel):
    """
    The main Mask2Former model which combines pixel-level processing and transformer-based decoding to produce final predictions.
    """
    
    main_input_name = "pixel_values"
    def __init__(self, config: Mask2FormerConfig):
        """
        Initializes the Mask2FormerModel with the given configuration.

        Args:
            config (Mask2FormerConfig): Configuration object containing parameters for the model.
        """
        
        super().__init__(config)
        self.pixel_level_module = Mask2FormerPixelLevelModule(config)
        self.transformer_module = Mask2FormerTransformerModule(in_features=config.feature_size, config=config)
        self.post_init()

    def forward(
        self,
        pixel_values: Tensor,
        class_names: List[list[str]],
        pixel_mask: Optional[Tensor] = None,
        img_bboxes: Optional[Tensor] = None,
        output_hidden_states: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Mask2FormerModelOutput:
        """
        Forward pass through the Mask2Former model.

        Args:
            pixel_values (Tensor): Pixel values tensor with shape (batch_size, num_channels, height, width).
            class_names (List[list[str]]): A nested list of class names for generating semantic queries.
            pixel_mask (Optional[Tensor], optional): Pixel mask tensor with shape (batch_size, height, width). Defaults to None.
            img_bboxes (Optional[Tensor], optional): Image bounding boxes tensor. Defaults to None.
            output_hidden_states (Optional[bool], optional): Whether to return hidden states. Defaults to None.
            output_attentions (Optional[bool], optional): Whether to return attentions. Defaults to None.
            return_dict (Optional[bool], optional): Whether to return a dictionary or a tuple. Defaults to None.

        Returns:
            Mask2FormerModelOutput: An object containing various outputs from the model including hidden states, attentions, etc.
        """
        
        # Set default values for optional parameters if not provided
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        batch_size, _, height, width = pixel_values.shape

        # If no pixel mask is provided, create a default one
        if pixel_mask is None:
            pixel_mask = torch.ones((batch_size, 1, height, width), device=pixel_values.device)

        # Process pixel-level features
        pixel_level_module_output = self.pixel_level_module(
            pixel_values=pixel_values, 
            img_bboxes=img_bboxes, 
            output_hidden_states=output_hidden_states
        )

        # Pass processed features through the transformer module
        transformer_module_output = self.transformer_module(
            multi_scale_features=pixel_level_module_output.decoder_hidden_states,
            mask_features=pixel_level_module_output.decoder_mask_features,
            class_names=class_names,
            pixel_mask=pixel_mask,
            img_bboxes=img_bboxes,
            output_hidden_states=True,
            output_attentions=output_attentions,
        )
        
        # Prepare the final output
        output = Mask2FormerModelOutput(
            pixel_encoder_hidden_states=pixel_level_module_output.encoder_hidden_states,
            pixel_decoder_hidden_states=pixel_level_module_output.decoder_hidden_states,
            pixel_decoder_mask_features=pixel_level_module_output.decoder_mask_features,
            transformer_decoder_instance_states=transformer_module_output.all_instance_states,
            transformer_decoder_semantic_states=transformer_module_output.all_semantic_states,
            transformer_decoder_instance_masks=transformer_module_output.all_instance_masks,
            transformer_decoder_semantic_masks=transformer_module_output.all_semantic_masks,
            transformer_decoder_bbox_predictions=transformer_module_output.all_bbox_predictions,
            transformer_decoder_cate_predictions=transformer_module_output.all_cate_predictions,
        )
        
        if not return_dict:
            output = tuple(v for v in output.values() if v is not None)

        return output


# Mask2FormerForUniversalSegmentation
class Mask2FormerForUniversalSegmentation(Mask2FormerPreTrainedModel):
    """
    A specialized model designed for universal segmentation tasks based on the Mask2Former architecture. 
    This class extends the basic Mask2Former model by incorporating loss computation and auxiliary logits extraction.
    """
    
    main_input_name = "pixel_values"

    def __init__(self, config: Mask2FormerConfig):
        """
        Initializes the Mask2FormerForUniversalSegmentation model with the provided configuration.

        Args:
            config (Mask2FormerConfig): Configuration object containing parameters specific to the model such as weights for different losses.
        """
        
        super().__init__(config)
        
        # Initialize the base Mask2Former model structure
        self.model = Mask2FormerModel(config)

        # Define a dictionary of weights for various types of losses used in the segmentation task
        self.weight_dict: Dict[str, float] = {
            "loss_class": config.class_weight,
            "loss_sml1":  config.sml1_weight,
            "loss_diou":  config.diou_weight,
            "loss_dice":  config.dice_weight,
            "loss_focal": config.focal_weight,
        }
        
        # Compute layer-wise weights for auxiliary losses; these are inversely proportional to decoder layer index
        self.layer_weight = [(i+1) / config.decoder_layers for i in range(config.decoder_layers)]
        
        # Instantiate the loss function used for computing the overall loss
        self.criterion = Mask2FormerLoss(config=config)
        self.post_init()


    def get_loss_dict(
        self,
        class_names: List[list[str]],
        true_instance_masks: Optional[List[Tensor]] = None,
        true_instance_bboxes: Optional[List[Tensor]] = None,
        true_instance_labels: Optional[List[Tensor]] = None,
        true_semantic_masks: Optional[List[Tensor]] = None,
        pred_instance_masks: Optional[Tensor] = None,
        pred_instance_bboxes: Optional[Tensor] = None,
        pred_instance_labels: Optional[Tensor] = None,
        pred_semantic_masks: Optional[Tensor] = None,
        auxiliary_predictions: Optional[List[Dict]] = None,
    ) -> Dict[str, Tensor]:
        """
        Computes the loss dictionary based on predictions and ground truths.

        Args:
            class_names (List[list[str]]): Class names for generating semantic queries.
            true_instance_masks (Optional[List[Tensor]], optional): Ground truth instance masks.
            true_instance_bboxes (Optional[List[Tensor]], optional): Ground truth bounding boxes.
            true_instance_labels (Optional[List[Tensor]], optional): Ground truth labels.
            true_semantic_masks (Optional[List[Tensor]], optional): Ground truth semantic masks.
            pred_instance_masks (Optional[Tensor], optional): Predicted instance masks.
            pred_instance_bboxes (Optional[Tensor], optional): Predicted bounding boxes.
            pred_instance_labels (Optional[Tensor], optional): Predicted labels.
            pred_semantic_masks (Optional[Tensor], optional): Predicted semantic masks.
            auxiliary_predictions (Optional[List[Dict]], optional): Auxiliary predictions from intermediate layers.

        Returns:
            Dict[str, Tensor]: Dictionary containing various computed losses.
        """
        
        # Call the loss criterion with all relevant inputs to compute individual losses
        loss_dict: Dict[str, Tensor] = self.criterion(
            class_names = class_names,
            true_instance_masks = true_instance_masks,
            true_instance_bboxes = true_instance_bboxes,
            true_instance_labels = true_instance_labels,
            true_semantic_masks = true_semantic_masks,
            pred_instance_masks = pred_instance_masks,
            pred_instance_bboxes = pred_instance_bboxes,
            pred_instance_labels = pred_instance_labels,
            pred_semantic_masks = pred_semantic_masks,
            auxiliary_predictions = auxiliary_predictions,
        )

        # Apply weight adjustments to each loss component according to predefined weights
        for loss_key, loss in loss_dict.items():
            for key, weight in self.weight_dict.items():
                if key in loss_key:
                    loss *= weight
            # for i in range(len(self.layer_weight)):
            #     if loss_key.endswith("_" + str(i)):
            #         loss *= self.layer_weight[i]

        return loss_dict

    def get_loss(self, loss_dict: Dict[str, Tensor]) -> Tensor:
        """
        Sums up all individual losses in the loss dictionary.

        Args:
            loss_dict (Dict[str, Tensor]): Dictionary containing various losses.

        Returns:
            Tensor: Total loss.
        """
        
        return sum(loss_dict.values())

    def get_auxiliary_logits(self, outputs):
        """
        Extracts auxiliary logits from the model's output for intermediate supervision.

        Args:
            outputs: Model outputs containing transformer decoder states.

        Returns:
            List[Dict]: List of dictionaries containing auxiliary logits.
        """
        
        auxiliary_logits = []
        for idx in range(len(outputs.transformer_decoder_instance_masks) - 1):
            auxiliary_logit = {"pred_instance_masks": outputs.transformer_decoder_instance_masks[idx], 
                               "pred_instance_bboxes": outputs.transformer_decoder_bbox_predictions[idx], 
                               "pred_instance_labels": outputs.transformer_decoder_cate_predictions[idx], 
                               "pred_semantic_masks": outputs.transformer_decoder_semantic_masks[idx],}
            auxiliary_logits.append(auxiliary_logit)
            
        return auxiliary_logits


    def forward(
        self,
        pixel_values: Tensor,
        class_names: List[list[str]], 
        pixel_mask: Optional[Tensor] = None,
        img_bboxes: Optional[Tensor] = None,
        instance_masks: Optional[List[Tensor]] = None,
        instance_bboxes: Optional[List[Tensor]] = None,
        instance_labels: Optional[List[Tensor]] = None,
        semantic_masks: Optional[List[Tensor]] = None,
        output_hidden_states: Optional[bool] = None,
        output_auxiliary_logits: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Mask2FormerForUniversalSegmentationOutput:
        """
        Forward pass through the Mask2FormerForUniversalSegmentation model.

        Args:
            pixel_values (Tensor): Input image tensor.
            class_names (List[list[str]]): Nested list of class names for generating semantic queries.
            pixel_mask (Optional[Tensor], optional): Pixel mask tensor. Defaults to None.
            img_bboxes (Optional[Tensor], optional): Image bounding boxes tensor. Defaults to None.
            instance_masks (Optional[List[Tensor]], optional): Ground truth instance masks. Defaults to None.
            instance_bboxes (Optional[List[Tensor]], optional): Ground truth bounding boxes. Defaults to None.
            instance_labels (Optional[List[Tensor]], optional): Ground truth labels. Defaults to None.
            semantic_masks (Optional[List[Tensor]], optional): Ground truth semantic masks. Defaults to None.
            output_hidden_states (Optional[bool], optional): Whether to return hidden states. Defaults to None.
            output_auxiliary_logits (Optional[bool], optional): Whether to return auxiliary logits. Defaults to None.
            output_attentions (Optional[bool], optional): Whether to return attention tensors. Defaults to None.
            return_dict (Optional[bool], optional): Whether to return a dictionary or a tuple. Defaults to None.

        Returns:
            Mask2FormerForUniversalSegmentationOutput: Output object containing the model's predictions and losses.
        """
        
        # Set default values for certain flags if not explicitly provided
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
        
        # Perform the forward pass through the base model
        outputs = self.model(
            pixel_values = pixel_values,
            class_names = class_names,
            pixel_mask = pixel_mask,
            img_bboxes=img_bboxes,
            output_hidden_states = output_hidden_states or self.config.use_auxiliary_loss,
            output_attentions = output_attentions,
            return_dict = True,
        )
        
        # If ground truth data is available, compute the loss
        loss, loss_dict = None, None
        if instance_masks is not None and instance_labels is not None:
            loss_dict = self.get_loss_dict(
                class_names = class_names,
                true_instance_masks  = instance_masks,
                true_instance_bboxes = instance_bboxes,
                true_instance_labels = instance_labels,
                true_semantic_masks  = semantic_masks, 
                pred_instance_masks  = outputs.transformer_decoder_instance_masks[-1],
                pred_instance_bboxes = outputs.transformer_decoder_bbox_predictions[-1],
                pred_instance_labels = outputs.transformer_decoder_cate_predictions[-1],
                pred_semantic_masks  = outputs.transformer_decoder_semantic_masks[-1],
                auxiliary_predictions = self.get_auxiliary_logits(outputs),
            )
            loss = self.get_loss(loss_dict)
        
        # Prepare the final output object with necessary information including losses and predictions
        output = Mask2FormerForUniversalSegmentationOutput(
            loss=loss,
            pixel_encoder_hidden_states=outputs.pixel_encoder_hidden_states,
            pixel_decoder_hidden_states=outputs.pixel_decoder_hidden_states,
            pixel_decoder_mask_features=outputs.pixel_decoder_mask_features,
            transformer_decoder_instance_states=outputs.transformer_decoder_instance_states,
            transformer_decoder_semantic_states=outputs.transformer_decoder_semantic_states,
            transformer_decoder_instance_masks=outputs.transformer_decoder_instance_masks,
            transformer_decoder_semantic_masks=outputs.transformer_decoder_semantic_masks,
            transformer_decoder_bbox_predictions=outputs.transformer_decoder_bbox_predictions,
            transformer_decoder_cate_predictions=outputs.transformer_decoder_cate_predictions,
        )
        
        # Depending on the return_dict flag, return either a dictionary or a tuple of values
        if not return_dict:
            output = tuple(v for v in output.values() if v is not None)
            if loss is not None:
                output = (loss) + output
                
        return output
